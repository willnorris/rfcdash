<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml"><!-- Mirrored from tools.ietf.org/html/rfc4313 by HTTrack Website Copier/3.x [XR&CO'2014], Tue, 06 Mar 2018 23:18:41 GMT --><!-- Added by HTTrack --><head><meta content="text/html;charset=utf-8" http-equiv="content-type"/><!-- /Added by HTTrack -->

    <meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
    <meta content="index,follow" name="robots"/>
    <meta content="rfcmarkup version 1.126" name="creator"/>
    <link href="http://purl.org/dc/elements/1.1/" rel="schema.DC"/>
<meta content="draft-burger-speechsc-reqts" name="DC.Relation.Replaces"/>
<meta content="urn:ietf:rfc:4313" name="DC.Identifier"/>
<meta content="December, 2005" name="DC.Date.Issued"/>
<meta content="David R Oran &lt;oran@cisco.com&gt;" name="DC.Creator"/>
<meta content="This document outlines the needs and requirements for a protocol to
control distributed speech processing of audio streams. By speech
processing, this document specifically means automatic speech
recognition (ASR), speaker recognition -- which includes both speaker
identification (SI) and speaker verification (SV) -- and text-to-
speech (TTS). Other IETF protocols, such as SIP and Real Time
Streaming Protocol (RTSP), address rendezvous and control for
generalized media streams. However, speech processing presents
additional requirements that none of the extant IETF protocols
address. This memo provides information for the Internet community." name="DC.Description.Abstract"/>
<meta content="Requirements for Distributed Control of Automatic Speech Recognition (ASR), Speaker Identification/Speaker Verification (SI/SV), and Text-to-Speech (TTS) Resources" name="DC.Title"/>

    <link href="https://tools.ietf.org/images/rfc.png" rel="icon" type="image/png"/>
    <link href="https://tools.ietf.org/images/rfc.png" rel="shortcut icon" type="image/png"/>
    <title>RFC 4313 - Requirements for Distributed Control of Automatic Speech Recognition (ASR), Speaker Identification/Speaker Verification (SI/SV), and Text-to-Speech (TTS) Resources</title>
    
    
    <style type="text/css">
	@media only screen 
	  and (min-width: 992px)
	  and (max-width: 1199px) {
	    body { font-size: 14pt; }
            div.content { width: 96ex; margin: 0 auto; }
        }
	@media only screen 
	  and (min-width: 768px)
	  and (max-width: 991px) {
            body { font-size: 14pt; }
            div.content { width: 96ex; margin: 0 auto; }
        }
	@media only screen 
	  and (min-width: 480px)
	  and (max-width: 767px) {
            body { font-size: 11pt; }
            div.content { width: 96ex; margin: 0 auto; }
        }
	@media only screen 
	  and (max-width: 479px) {
            body { font-size: 8pt; }
            div.content { width: 96ex; margin: 0 auto; }
        }
	@media only screen 
	  and (min-device-width : 375px) 
	  and (max-device-width : 667px) {
            body { font-size: 9.5pt; }
            div.content { width: 96ex; margin: 0 1px; }
        }
	@media only screen 
	  and (min-device-width: 1200px) {
            body { font-size: 10pt; margin: 0 4em; }
            div.content { width: 96ex; margin: 0; }
        }
        h1, h2, h3, h4, h5, h6, .h1, .h2, .h3, .h4, .h5, .h6 {
	    font-weight: bold;
            line-height: 0pt;
            display: inline;
            white-space: pre;
            font-family: monospace;
            font-size: 1em;
	    font-weight: bold;
        }
        pre {
            font-size: 1em;
            margin-top: 0px;
            margin-bottom: 0px;
        }
	.pre {
	    white-space: pre;
	    font-family: monospace;
	}
	.header{
	    font-weight: bold;
	}
        .newpage {
            page-break-before: always;
        }
        .invisible {
            text-decoration: none;
            color: white;
        }
        a.selflink {
          color: black;
          text-decoration: none;
        }
        @media print {
            body {
                font-family: monospace;
                font-size: 10.5pt;
            }
            h1, h2, h3, h4, h5, h6 {
                font-size: 1em;
            }
        
            a:link, a:visited {
                color: inherit;
                text-decoration: none;
            }
            .noprint {
                display: none;
            }
        }
	@media screen {
	    .grey, .grey a:link, .grey a:visited {
		color: #777;
	    }
            .docinfo {
                background-color: #EEE;
            }
            .top {
                border-top: 7px solid #EEE;
            }
            .bgwhite  { background-color: white; }
            .bgred    { background-color: #F44; }
            .bggrey   { background-color: #666; }
            .bgbrown  { background-color: #840; }            
            .bgorange { background-color: #FA0; }
            .bgyellow { background-color: #EE0; }
            .bgmagenta{ background-color: #F4F; }
            .bgblue   { background-color: #66F; }
            .bgcyan   { background-color: #4DD; }
            .bggreen  { background-color: #4F4; }

            .legend   { font-size: 90%; }
            .cplate   { font-size: 70%; border: solid grey 1px; }
	}
    </style>
    <!--[if IE]>
    <style>
    body {
       font-size: 13px;
       margin: 10px 10px;
    }
    </style>
    <![endif]-->

    <script type="text/javascript"><!--
    function addHeaderTags() {
	var spans = document.getElementsByTagName("span");
	for (var i=0; i < spans.length; i++) {
	    var elem = spans[i];
	    if (elem) {
		var level = elem.getAttribute("class");
                if (level == "h1" || level == "h2" || level == "h3" || level == "h4" || level == "h5" || level == "h6") {
                    elem.innerHTML = "<"+level+">"+elem.innerHTML+"</"+level+">";		
                }
	    }
	}
    }
    var legend_html = "Colour legend:<br />      <table>         <tr><td>Unknown:</td>                   <td><span class='cplate bgwhite'>&nbsp;&nbsp;&nbsp;&nbsp;</span></td></tr>         <tr><td>Draft:</td>                     <td><span class='cplate bgred'>&nbsp;&nbsp;&nbsp;&nbsp;</span></td></tr>         <tr><td>Informational:</td>             <td><span class='cplate bgorange'>&nbsp;&nbsp;&nbsp;&nbsp;</span></td></tr>         <tr><td>Experimental:</td>              <td><span class='cplate bgyellow'>&nbsp;&nbsp;&nbsp;&nbsp;</span></td></tr>         <tr><td>Best Common Practice:</td>      <td><span class='cplate bgmagenta'>&nbsp;&nbsp;&nbsp;&nbsp;</span></td></tr>         <tr><td>Proposed Standard:</td>         <td><span class='cplate bgblue'>&nbsp;&nbsp;&nbsp;&nbsp;</span></td></tr>         <tr><td>Draft Standard (old designation):</td> <td><span class='cplate bgcyan'>&nbsp;&nbsp;&nbsp;&nbsp;</span></td></tr>         <tr><td>Internet Standard:</td>         <td><span class='cplate bggreen'>&nbsp;&nbsp;&nbsp;&nbsp;</span></td></tr>         <tr><td>Historic:</td>                  <td><span class='cplate bggrey'>&nbsp;&nbsp;&nbsp;&nbsp;</span></td></tr>         <tr><td>Obsolete:</td>                  <td><span class='cplate bgbrown'>&nbsp;&nbsp;&nbsp;&nbsp;</span></td></tr>     </table>";
    function showElem(id) {
        var elem = document.getElementById(id);
        elem.innerHTML = eval(id+"_html");
        elem.style.visibility='visible';
    }
    function hideElem(id) {
        var elem = document.getElementById(id);
        elem.style.visibility='hidden';        
        elem.innerHTML = "";
    }
    // -->
    </script>
</head>
<body onload="addHeaderTags()">
  <div class="content">
   <div style="height: 13px;">
      <div class="pre noprint docinfo bgorange" onclick="showElem('legend');" onmouseout="hideElem('legend')" onmouseover="this.style.cursor='pointer';" style="height: 6px; position: absolute;" title="Click for colour legend.">                                                                        </div>
      <div class="docinfo noprint pre legend" id="legend" onmouseout="hideElem('legend');" onmouseover="showElem('legend');" style="position:absolute; top: 4px; left: 4ex; visibility:hidden; background-color: white; padding: 4px 9px 5px 7px; border: solid #345 1px; ">
      </div>
   </div>
<span class="pre noprint docinfo top">[<a href="index.html" title="Document search and retrieval page">Docs</a>] [<a href="https://tools.ietf.org/rfc/rfc4313.txt" title="Plaintext version of this document">txt</a>|<a href="https://tools.ietf.org/pdf/rfc4313" title="PDF version of this document">pdf</a>] [<a href="https://tools.ietf.org/html/draft-ietf-speechsc-reqts" title="draft-ietf-speechsc-reqts">draft-ietf-spee...</a>] [<a href="https://datatracker.ietf.org/doc/rfc4313" title="IESG Datatracker information for this document">Tracker</a>] [<a href="https://tools.ietf.org/rfcdiff?difftype=--hwdiff&amp;url2=rfc4313" title="Inline diff (wdiff)">Diff1</a>] [<a href="https://tools.ietf.org/rfcdiff?url2=rfc4313" title="Side-by-side diff">Diff2</a>]         </span><br/>
<span class="pre noprint docinfo">                                                                        </span><br/>
<span class="pre noprint docinfo">                                                           INFORMATIONAL</span><br/>
<span class="pre noprint docinfo">                                                                        </span><br/>
<pre>Network Working Group                                            D. Oran
Request for Comments: 4313                           Cisco Systems, Inc.
Category: Informational                                    December 2005


                <span class="h1">Requirements for Distributed Control of</span>
                  <span class="h1">Automatic Speech Recognition (ASR),</span>
       <span class="h1">Speaker Identification/Speaker Verification (SI/SV), and</span>
                     Text-to-Speech (TTS) Resources

Status of this Memo

   This memo provides information for the Internet community.  It does
   not specify an Internet standard of any kind.  Distribution of this
   memo is unlimited.

Copyright Notice

   Copyright (C) The Internet Society (2005).

Abstract

   This document outlines the needs and requirements for a protocol to
   control distributed speech processing of audio streams.  By speech
   processing, this document specifically means automatic speech
   recognition (ASR), speaker recognition -- which includes both speaker
   identification (SI) and speaker verification (SV) -- and
   text-to-speech (TTS).  Other IETF protocols, such as SIP and Real
   Time Streaming Protocol (RTSP), address rendezvous and control for
   generalized media streams.  However, speech processing presents
   additional requirements that none of the extant IETF protocols
   address.

Table of Contents

   <a href="#section-1">1</a>. Introduction ....................................................<a href="#page-3">3</a>
      <a href="#section-1.1">1.1</a>. Document Conventions .......................................<a href="#page-3">3</a>
   <a href="#section-2">2</a>. SPEECHSC Framework ..............................................<a href="#page-4">4</a>
      <a href="#section-2.1">2.1</a>. TTS Example ................................................<a href="#page-5">5</a>
      <a href="#section-2.2">2.2</a>. Automatic Speech Recognition Example .......................<a href="#page-6">6</a>
      <a href="#section-2.3">2.3</a>. Speaker Identification example .............................<a href="#page-6">6</a>
   <a href="#section-3">3</a>. General Requirements ............................................<a href="#page-7">7</a>
      <a href="#section-3.1">3.1</a>. Reuse Existing Protocols ...................................<a href="#page-7">7</a>
      <a href="#section-3.2">3.2</a>. Maintain Existing Protocol Integrity .......................<a href="#page-7">7</a>
      <a href="#section-3.3">3.3</a>. Avoid Duplicating Existing Protocols .......................<a href="#page-7">7</a>
      <a href="#section-3.4">3.4</a>. Efficiency .................................................<a href="#page-8">8</a>
      <a href="#section-3.5">3.5</a>. Invocation of Services .....................................<a href="#page-8">8</a>
      <a href="#section-3.6">3.6</a>. Location and Load Balancing ................................<a href="#page-8">8</a>



<span class="grey">Oran                         Informational                      [Page 1]</span></pre>
<hr align="left" class="noprint" style="width: 96ex;"/><!--NewPage--><pre class="newpage"><a class="invisible" href="#page-2" id="page-2" name="page-2"> </a>
<span class="grey"><a href="rfc4313.html">RFC 4313</a>          Speech Services Control Requirements     December 2005</span>


      <a href="#section-3.7">3.7</a>. Multiple Services ..........................................<a href="#page-8">8</a>
      <a href="#section-3.8">3.8</a>. Multiple Media Sessions ....................................<a href="#page-8">8</a>
      <a href="#section-3.9">3.9</a>. Users with Disabilities ....................................<a href="#page-9">9</a>
      3.10. Identification of Process That Produced Media or
            Control Output ............................................<a href="#page-9">9</a>
   <a href="#section-4">4</a>. TTS Requirements ................................................<a href="#page-9">9</a>
      <a href="#section-4.1">4.1</a>. Requesting Text Playback ...................................<a href="#page-9">9</a>
      <a href="#section-4.2">4.2</a>. Text Formats ...............................................<a href="#page-9">9</a>
           <a href="#section-4.2.1">4.2.1</a>. Plain Text ..........................................<a href="#page-9">9</a>
           <a href="#section-4.2.2">4.2.2</a>. SSML ................................................<a href="#page-9">9</a>
           <a href="#section-4.2.3">4.2.3</a>. Text in Control Channel ............................<a href="#page-10">10</a>
           <a href="#section-4.2.4">4.2.4</a>. Document Type Indication ...........................<a href="#page-10">10</a>
      <a href="#section-4.3">4.3</a>. Control Channel ...........................................<a href="#page-10">10</a>
      <a href="#section-4.4">4.4</a>. Media Origination/Termination by Control Elements .........<a href="#page-10">10</a>
      <a href="#section-4.5">4.5</a>. Playback Controls .........................................<a href="#page-10">10</a>
      <a href="#section-4.6">4.6</a>. Session Parameters ........................................<a href="#page-11">11</a>
      <a href="#section-4.7">4.7</a>. Speech Markers ............................................<a href="#page-11">11</a>
   <a href="#section-5">5</a>. ASR Requirements ...............................................<a href="#page-11">11</a>
      <a href="#section-5.1">5.1</a>. Requesting Automatic Speech Recognition ...................<a href="#page-11">11</a>
      <a href="#section-5.2">5.2</a>. XML .......................................................<a href="#page-11">11</a>
      <a href="#section-5.3">5.3</a>. Grammar Requirements ......................................<a href="#page-12">12</a>
           <a href="#section-5.3.1">5.3.1</a>. Grammar Specification ..............................<a href="#page-12">12</a>
           <a href="#section-5.3.2">5.3.2</a>. Explicit Indication of Grammar Format ..............<a href="#page-12">12</a>
           <a href="#section-5.3.3">5.3.3</a>. Grammar Sharing ....................................<a href="#page-12">12</a>
      <a href="#section-5.4">5.4</a>. Session Parameters ........................................<a href="#page-12">12</a>
      <a href="#section-5.5">5.5</a>. Input Capture .............................................<a href="#page-12">12</a>
   <a href="#section-6">6</a>. Speaker Identification and Verification Requirements ...........<a href="#page-13">13</a>
      <a href="#section-6.1">6.1</a>. Requesting SI/SV ..........................................<a href="#page-13">13</a>
      <a href="#section-6.2">6.2</a>. Identifiers for SI/SV .....................................<a href="#page-13">13</a>
      <a href="#section-6.3">6.3</a>. State for Multiple Utterances .............................<a href="#page-13">13</a>
      <a href="#section-6.4">6.4</a>. Input Capture .............................................<a href="#page-13">13</a>
      <a href="#section-6.5">6.5</a>. SI/SV Functional Extensibility ............................<a href="#page-13">13</a>
   <a href="#section-7">7</a>. Duplexing and Parallel Operation Requirements ..................<a href="#page-13">13</a>
      <a href="#section-7.1">7.1</a>. Full Duplex Operation .....................................<a href="#page-14">14</a>
      <a href="#section-7.2">7.2</a>. Multiple Services in Parallel .............................<a href="#page-14">14</a>
      <a href="#section-7.3">7.3</a>. Combination of Services ...................................<a href="#page-14">14</a>
   <a href="#section-8">8</a>. Additional Considerations (Non-Normative) ......................<a href="#page-14">14</a>
   <a href="#section-9">9</a>. Security Considerations ........................................<a href="#page-15">15</a>
      <a href="#section-9.1">9.1</a>. SPEECHSC Protocol Security ................................<a href="#page-15">15</a>
      <a href="#section-9.2">9.2</a>. Client and Server Implementation and Deployment ...........<a href="#page-16">16</a>
      <a href="#section-9.3">9.3</a>. Use of SPEECHSC for Security Functions ....................<a href="#page-16">16</a>
   <a href="#section-10">10</a>. Acknowledgements ..............................................<a href="#page-17">17</a>
   <a href="#section-11">11</a>. References ....................................................<a href="#page-18">18</a>
      <a href="#section-11.1">11.1</a>. Normative References .....................................<a href="#page-18">18</a>
      <a href="#section-11.2">11.2</a>. Informative References ...................................<a href="#page-18">18</a>






<span class="grey">Oran                         Informational                      [Page 2]</span></pre>
<hr align="left" class="noprint" style="width: 96ex;"/><!--NewPage--><pre class="newpage"><a class="invisible" href="#page-3" id="page-3" name="page-3"> </a>
<span class="grey"><a href="rfc4313.html">RFC 4313</a>          Speech Services Control Requirements     December 2005</span>


<span class="h2"><a class="dashAnchor" name="//apple_ref/Section/1.%20%20Introduction"></a><a class="selflink" href="#section-1" name="section-1">1</a>.  Introduction</span>

   There are multiple IETF protocols for establishment and termination
   of media sessions (SIP [<a href="#ref-6" title='"SIP: Session Initiation Protocol"'>6</a>]), low-level media control (Media Gateway
   Control Protocol (MGCP) [<a href="#ref-7" title='"Media Gateway Control Protocol (MGCP) Version 1.0"'>7</a>] and Media Gateway Controller (MEGACO)
   [<a href="#ref-8" title='"Gateway Control Protocol Version 1"'>8</a>]), and media record and playback (RTSP [<a href="#ref-9" title='"Real Time Streaming Protocol (RTSP)"'>9</a>]).  This document
   focuses on requirements for one or more protocols to support the
   control of network elements that perform Automated Speech Recognition
   (ASR), speaker identification or verification (SI/SV), and rendering
   text into audio, also known as Text-to-Speech (TTS).  Many multimedia
   applications can benefit from having automatic speech recognition
   (ASR) and text-to-speech (TTS) processing available as a distributed,
   network resource.  This requirements document limits its focus to the
   distributed control of ASR, SI/SV, and TTS servers.

   There is a broad range of systems that can benefit from a unified
   approach to control of TTS, ASR, and SI/SV.  These include
   environments such as Voice over IP (VoIP) gateways to the Public
   Switched Telephone Network (PSTN), IP telephones, media servers, and
   wireless mobile devices that obtain speech services via servers on
   the network.

   To date, there are a number of proprietary ASR and TTS APIs, as well
   as two IETF documents that address this problem [<a href="#ref-13" title='"Service Location Protocol, Version 2"'>13</a>], [<a href="#ref-14" title='"A DNS RR for specifying the location of services (DNS SRV)"'>14</a>].  However,
   there are serious deficiencies to the existing documents.  In
   particular, they mix the semantics of existing protocols yet are
   close enough to other protocols as to be confusing to the
   implementer.

   This document sets forth requirements for protocols to support
   distributed speech processing of audio streams.  For simplicity, and
   to remove confusion with existing protocol proposals, this document
   presents the requirements as being for a "framework" that addresses
   the distributed control of speech resources.  It refers to such a
   framework as "SPEECHSC", for Speech Services Control.

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/1.1.%20%20Document%20Conventions"></a><a class="selflink" href="#section-1.1" name="section-1.1">1.1</a>.  Document Conventions</span>

   In this document, the key words "MUST", "MUST NOT", "REQUIRED",
   "SHALL", "SHALL NOT", "SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY",
   and "OPTIONAL" are to be interpreted as described in <a href="rfc2119.html">RFC 2119</a> [<a href="#ref-3" title='"Key words for use in RFCs to Indicate Requirement Levels"'>3</a>].










<span class="grey">Oran                         Informational                      [Page 3]</span></pre>
<hr align="left" class="noprint" style="width: 96ex;"/><!--NewPage--><pre class="newpage"><a class="invisible" href="#page-4" id="page-4" name="page-4"> </a>
<span class="grey"><a href="rfc4313.html">RFC 4313</a>          Speech Services Control Requirements     December 2005</span>


<span class="h2"><a class="dashAnchor" name="//apple_ref/Section/2.%20%20SPEECHSC%20Framework"></a><a class="selflink" href="#section-2" name="section-2">2</a>.  SPEECHSC Framework</span>

   Figure 1 below shows the SPEECHSC framework for speech processing.

                          +-------------+
                          | Application |
                          |   Server    |\
                          +-------------+ \ SPEECHSC
            SIP, VoiceXML,  /              \
             etc.          /                \
           +------------+ /                  \    +-------------+
           |   Media    |/       SPEECHSC     \---| ASR, SI/SV, |
           | Processing |-------------------------| and/or TTS  |
       RTP |   Entity   |           RTP           |    Server   |
      =====|            |=========================|             |
           +------------+                         +-------------+

                       Figure 1: SPEECHSC Framework

   The "Media Processing Entity" is a network element that processes
   media.  It may be a pure media handler, or it may also have an
   associated SIP user agent, VoiceXML browser, or other control entity.
   The "ASR, SI/SV, and/or TTS Server" is a network element that
   performs the back-end speech processing.  It may generate an RTP
   stream as output based on text input (TTS) or return recognition
   results in response to an RTP stream as input (ASR, SI/SV).  The
   "Application Server" is a network element that instructs the Media
   Processing Entity on what transformations to make to the media
   stream.  Those instructions may be established via a session protocol
   such as SIP, or provided via a client/server exchange such as
   VoiceXML.  The framework allows either the Media Processing Entity or
   the Application Server to control the ASR or TTS Server using
   SPEECHSC as a control protocol, which accounts for the SPEECHSC
   protocol appearing twice in the diagram.

   Physical embodiments of the entities can reside in one physical
   instance per entity, or some combination of entities.  For example, a
   VoiceXML [<a href="#ref-11" title='"Voice Extensible Markup Language (VoiceXML) Version 2.0"'>11</a>] gateway may combine the ASR and TTS functions on the
   same platform as the Media Processing Entity.  Note that VoiceXML
   gateways themselves are outside the scope of this protocol.
   Likewise, one can combine the Application Server and Media Processing
   Entity, as would be the case in an interactive voice response (IVR)
   platform.

   One can also decompose the Media Processing Entity into an entity
   that controls media endpoints and entities that process media
   directly.  Such would be the case with a decomposed gateway using
   MGCP or MEGACO.  However, this decomposition is again orthogonal to



<span class="grey">Oran                         Informational                      [Page 4]</span></pre>
<hr align="left" class="noprint" style="width: 96ex;"/><!--NewPage--><pre class="newpage"><a class="invisible" href="#page-5" id="page-5" name="page-5"> </a>
<span class="grey"><a href="rfc4313.html">RFC 4313</a>          Speech Services Control Requirements     December 2005</span>


   the scope of SPEECHSC.  The following subsections provide a number of
   example use cases of the SPEECHSC, one each for TTS, ASR, and SI/SV.
   They are intended to be illustrative only, and not to imply any
   restriction on the scope of the framework or to limit the
   decomposition or configuration to that shown in the example.

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/2.1.%20%20TTS%20Example"></a><a class="selflink" href="#section-2.1" name="section-2.1">2.1</a>.  TTS Example</span>

   This example illustrates a simple usage of SPEECHSC to provide a
   Text-to-Speech service for playing announcements to a user on a phone
   with no display for textual error messages.  The example scenario is
   shown below in Figure 2.  In the figure, the VoIP gateway acts as
   both the Media Processing Entity and the Application Server of the
   SPEECHSC framework in Figure 1.

                                      +---------+
                                     _|   SIP   |
                                   _/ |  Server |
                +-----------+  SIP/   +---------+
                |           |  _/
    +-------+   |   VoIP    |_/
    | POTS  |___| Gateway   |   RTP   +---------+
    | Phone |   | (SIP UA)  |=========|         |
    +-------+   |           |\_       | SPEECHSC|
                +-----------+  \      |   TTS   |
                                \__   |  Server |
                             SPEECHSC |         |
                                    \_|         |
                                      +---------+

               Figure 2: Text-to-Speech Example of SPEECHSC

   The Plain Old Telephone Service (POTS) phone on the left attempts to
   make a phone call.  The VoIP gateway, acting as a SIP UA, tries to
   establish a SIP session to complete the call, but gets an error, such
   as a SIP "486 Busy Here" response.  Without SPEECHSC, the gateway
   would most likely just output a busy signal to the POTS phone.
   However, with SPEECHSC access to a TTS server, it can provide a
   spoken error message.  The VoIP gateway therefore constructs a text
   error string using information from the SIP messages, such as "Your
   call to 978-555-1212 did not go through because the called party was
   busy".  It then can use SPEECHSC to establish an association with a
   SPEECHSC server, open an RTP stream between itself and the server,
   and issue a TTS request for the error message, which will be played
   to the user on the POTS phone.






<span class="grey">Oran                         Informational                      [Page 5]</span></pre>
<hr align="left" class="noprint" style="width: 96ex;"/><!--NewPage--><pre class="newpage"><a class="invisible" href="#page-6" id="page-6" name="page-6"> </a>
<span class="grey"><a href="rfc4313.html">RFC 4313</a>          Speech Services Control Requirements     December 2005</span>


<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/2.2.%20%20Automatic%20Speech%20Recognition%20Example"></a><a class="selflink" href="#section-2.2" name="section-2.2">2.2</a>.  Automatic Speech Recognition Example</span>

   This example illustrates a VXML-enabled media processing entity and
   associated application server using the SPEECHSC framework to supply
   an ASR-based user interface through an Interactive Voice Response
   (IVR) system.  The example scenario is shown below in Figure 3.  The
   VXML-client corresponds to the "media processing entity", while the
   IVR application server corresponds to the "application server" of the
   SPEECHSC framework of Figure 1.

                                      +------------+
                                      |    IVR     |
                                     _|Application |
                               VXML_/ +------------+
                +-----------+  __/
                |           |_/       +------------+
    PSTN Trunk  |   VoIP    | SPEECHSC|            |
   =============| Gateway   |---------| SPEECHSC   |
                |(VXML voice|         |   ASR      |
                | browser)  |=========|  Server    |
                +-----------+   RTP   +------------+

              Figure 3: Automatic Speech Recognition Example

   In this example, users call into the service in order to obtain stock
   quotes.  The VoIP gateway answers their PSTN call.  An IVR
   application feeds VXML scripts to the gateway to drive the user
   interaction.  The VXML interpreter on the gateway directs the user's
   media stream to the SPEECHSC ASR server and uses SPEECHSC to control
   the ASR server.

   When, for example, the user speaks the name of a stock in response to
   an IVR prompt, the SPEECHSC ASR server attempts recognition of the
   name, and returns the results to the VXML gateway.  The VXML gateway,
   following standard VXML mechanisms, informs the IVR Application of
   the recognized result.  The IVR Application can then do the
   appropriate information lookup.  The answer, of course, can be sent
   back to the user using text-to-speech.  This example does not show
   this scenario, but it would work analogously to the scenario shown in
   section <a href="#section-2.1">Section 2.1</a>.

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/2.3.%20%20Speaker%20Identification%20example"></a><a class="selflink" href="#section-2.3" name="section-2.3">2.3</a>.  Speaker Identification example</span>

   This example illustrates using speaker identification to allow
   voice-actuated login to an IP phone.  The example scenario is shown
   below in Figure 4.  In the figure, the IP Phone acts as both the
   "Media Processing Entity" and the "Application Server" of the
   SPEECHSC framework in Figure 1.



<span class="grey">Oran                         Informational                      [Page 6]</span></pre>
<hr align="left" class="noprint" style="width: 96ex;"/><!--NewPage--><pre class="newpage"><a class="invisible" href="#page-7" id="page-7" name="page-7"> </a>
<span class="grey"><a href="rfc4313.html">RFC 4313</a>          Speech Services Control Requirements     December 2005</span>


   +-----------+         +---------+
   |           |   RTP   |         |
   |   IP      |=========| SPEECHSC|
   |  Phone    |         |   TTS   |
   |           |_________|  Server |
   |           | SPEECHSC|         |
   +-----------+         +---------+

                 Figure 4: Speaker Identification Example

   In this example, a user speaks into a SIP phone in order to get
   "logged in" to that phone to make and receive phone calls using his
   identity and preferences.  The IP phone uses the SPEECHSC framework
   to set up an RTP stream between the phone and the SPEECHSC SI/SV
   server and to request verification.  The SV server verifies the
   user's identity and returns the result, including the necessary login
   credentials, to the phone via SPEECHSC.  The IP Phone may use the
   identity directly to identify the user in outgoing calls, to fetch
   the user's preferences from a configuration server, or to request
   authorization from an Authentication, Authorization, and Accounting
   (AAA) server, in any combination.  Since this example uses SPEECHSC
   to perform a security-related function, be sure to note the
   associated material in <a href="#section-9">Section 9</a>.

<span class="h2"><a class="dashAnchor" name="//apple_ref/Section/3.%20%20General%20Requirements"></a><a class="selflink" href="#section-3" name="section-3">3</a>.  General Requirements</span>

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/3.1.%20%20Reuse%20Existing%20Protocols"></a><a class="selflink" href="#section-3.1" name="section-3.1">3.1</a>.  Reuse Existing Protocols</span>

   To the extent feasible, the SPEECHSC framework SHOULD use existing
   protocols.

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/3.2.%20%20Maintain%20Existing%20Protocol%20Integrity"></a><a class="selflink" href="#section-3.2" name="section-3.2">3.2</a>.  Maintain Existing Protocol Integrity</span>

   In meeting the requirement of <a href="#section-3.1">Section 3.1</a>, the SPEECHSC framework
   MUST NOT redefine the semantics of an existing protocol.  Said
   differently, we will not break existing protocols or cause
   backward-compatibility problems.

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/3.3.%20%20Avoid%20Duplicating%20Existing%20Protocols"></a><a class="selflink" href="#section-3.3" name="section-3.3">3.3</a>.  Avoid Duplicating Existing Protocols</span>

   To the extent feasible, SPEECHSC SHOULD NOT duplicate the
   functionality of existing protocols.  For example, network
   announcements using SIP [<a href="#ref-12" title='"Basic Network Media Services with SIP"'>12</a>] and RTSP [<a href="#ref-9" title='"Real Time Streaming Protocol (RTSP)"'>9</a>] already define how to
   request playback of audio.  The focus of SPEECHSC is new
   functionality not addressed by existing protocols or extending
   existing protocols within the strictures of the requirement in





<span class="grey">Oran                         Informational                      [Page 7]</span></pre>
<hr align="left" class="noprint" style="width: 96ex;"/><!--NewPage--><pre class="newpage"><a class="invisible" href="#page-8" id="page-8" name="page-8"> </a>
<span class="grey"><a href="rfc4313.html">RFC 4313</a>          Speech Services Control Requirements     December 2005</span>


   <a href="#section-3.2">Section 3.2</a>.  Where an existing protocol can be gracefully extended
   to support SPEECHSC requirements, such extensions are acceptable
   alternatives for meeting the requirements.

   As a corollary to this, the SPEECHSC should not require a separate
   protocol to perform functions that could be easily added into the
   SPEECHSC protocol (like redirecting media streams, or discovering
   capabilities), unless it is similarly easy to embed that protocol
   directly into the SPEECHSC framework.

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/3.4.%20%20Efficiency"></a><a class="selflink" href="#section-3.4" name="section-3.4">3.4</a>.  Efficiency</span>

   The SPEECHSC framework SHOULD employ protocol elements known to
   result in efficient operation.  Techniques to be considered include:

   o  Re-use of transport connections across sessions
   o  Piggybacking of responses on requests in the reverse direction
   o  Caching of state across requests

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/3.5.%20%20Invocation%20of%20Services"></a><a class="selflink" href="#section-3.5" name="section-3.5">3.5</a>.  Invocation of Services</span>

   The SPEECHSC framework MUST be compliant with the IAB Open Pluggable
   Edge Services (OPES) [<a href="#ref-4" title='"IAB Architectural and Policy Considerations for Open Pluggable Edge Services"'>4</a>] framework.  The applicability of the
   SPEECHSC protocol will therefore be specified as occurring between
   clients and servers at least one of which is operating directly on
   behalf of the user requesting the service.

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/3.6.%20%20Location%20and%20Load%20Balancing"></a><a class="selflink" href="#section-3.6" name="section-3.6">3.6</a>.  Location and Load Balancing</span>

   To the extent feasible, the SPEECHSC framework SHOULD exploit
   existing schemes for supporting service location and load balancing,
   such as the Service Location Protocol [<a href="#ref-13" title='"Service Location Protocol, Version 2"'>13</a>] or DNS SRV records [<a href="#ref-14" title='"A DNS RR for specifying the location of services (DNS SRV)"'>14</a>].
   Where such facilities are not deemed adequate, the SPEECHSC framework
   MAY define additional load balancing techniques.

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/3.7.%20%20Multiple%20Services"></a><a class="selflink" href="#section-3.7" name="section-3.7">3.7</a>.  Multiple Services</span>

   The SPEECHSC framework MUST permit multiple services to operate on a
   single media stream so that either the same or different servers may
   be performing speech recognition, speaker identification or
   verification, etc., in parallel.

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/3.8.%20%20Multiple%20Media%20Sessions"></a><a class="selflink" href="#section-3.8" name="section-3.8">3.8</a>.  Multiple Media Sessions</span>

   The SPEECHSC framework MUST allow a 1:N mapping between session and
   RTP channels.  For example, a single session may include an outbound
   RTP channel for TTS, an inbound for ASR, and a different inbound for
   SI/SV (e.g., if processed by different elements on the Media Resource



<span class="grey">Oran                         Informational                      [Page 8]</span></pre>
<hr align="left" class="noprint" style="width: 96ex;"/><!--NewPage--><pre class="newpage"><a class="invisible" href="#page-9" id="page-9" name="page-9"> </a>
<span class="grey"><a href="rfc4313.html">RFC 4313</a>          Speech Services Control Requirements     December 2005</span>


   Element).  Note: All of these can be described via SDP, so if SDP is
   utilized for media channel description, this requirement is met "for
   free".

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/3.9.%20%20Users%20with%20Disabilities"></a><a class="selflink" href="#section-3.9" name="section-3.9">3.9</a>.  Users with Disabilities</span>

   The SPEECHSC framework must have sufficient capabilities to address
   the critical needs of people with disabilities.  In particular, the
   set of requirements set forth in <a href="rfc3351.html">RFC 3351</a> [<a href="#ref-5" title='"User Requirements for the Session Initiation Protocol (SIP) in Support of Deaf, Hard of Hearing and Speech-impaired Individuals"'>5</a>] MUST be taken into
   account by the framework.  It is also important that implementers of
   SPEECHSC clients and servers be cognizant that some interaction
   modalities of SPEECHSC may be inconvenient or simply inappropriate
   for disabled users.  Hearing-impaired individuals may find TTS of
   limited utility.  Speech-impaired users may be unable to make use of
   ASR or SI/SV capabilities.  Therefore, systems employing SPEECHSC
   MUST provide alternative interaction modes or avoid the use of speech
   processing entirely.

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/3.10.%20%20Identification%20of%20Process%20That%20Produced%20Media%20or%20Control%20Output"></a><a class="selflink" href="#section-3.10" name="section-3.10">3.10</a>.  Identification of Process That Produced Media or Control Output</span>

   The client of a SPEECHSC operation SHOULD be able to ascertain via
   the SPEECHSC framework what speech process produced the output.  For
   example, an RTP stream containing the spoken output of TTS should be
   identifiable as TTS output, and the recognized utterance of ASR
   should be identifiable as having been produced by ASR processing.

<span class="h2"><a class="dashAnchor" name="//apple_ref/Section/4.%20%20TTS%20Requirements"></a><a class="selflink" href="#section-4" name="section-4">4</a>.  TTS Requirements</span>

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/4.1.%20%20Requesting%20Text%20Playback"></a><a class="selflink" href="#section-4.1" name="section-4.1">4.1</a>.  Requesting Text Playback</span>

   The SPEECHSC framework MUST allow a Media Processing Entity or
   Application Server, using a control protocol, to request the TTS
   Server to play back text as voice in an RTP stream.

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/4.2.%20%20Text%20Formats"></a><a class="selflink" href="#section-4.2" name="section-4.2">4.2</a>.  Text Formats</span>

<span class="h4"><a class="dashAnchor" name="//apple_ref/Section/4.2.1.%20%20Plain%20Text"></a><a class="selflink" href="#section-4.2.1" name="section-4.2.1">4.2.1</a>.  Plain Text</span>

   The SPEECHSC framework MAY assume that all TTS servers are capable of
   reading plain text.  For reading plain text, framework MUST allow the
   language and voicing to be indicated via session parameters.  For
   finer control over such properties, see [<a href="#ref-1" title='"Speech Synthesis Markup Language (SSML) Version 1.0"'>1</a>].

<span class="h4"><a class="dashAnchor" name="//apple_ref/Section/4.2.2.%20%20SSML"></a><a class="selflink" href="#section-4.2.2" name="section-4.2.2">4.2.2</a>.  SSML</span>

   The SPEECHSC framework MUST support Speech Synthesis Markup Language
   (SSML)[<a href="#ref-1" title='"Speech Synthesis Markup Language (SSML) Version 1.0"'>1</a>] &lt;speak&gt; basics, and SHOULD support other SSML tags.  The
   framework assumes all TTS servers are capable of reading SSML



<span class="grey">Oran                         Informational                      [Page 9]</span></pre>
<hr align="left" class="noprint" style="width: 96ex;"/><!--NewPage--><pre class="newpage"><a class="invisible" href="#page-10" id="page-10" name="page-10"> </a>
<span class="grey"><a href="rfc4313.html">RFC 4313</a>          Speech Services Control Requirements     December 2005</span>


   formatted text.  Internationalization of TTS in the SPEECHSC
   framework, including multi-lingual output within a single utterance,
   is accomplished via SSML xml:lang tags.

<span class="h4"><a class="dashAnchor" name="//apple_ref/Section/4.2.3.%20%20Text%20in%20Control%20Channel"></a><a class="selflink" href="#section-4.2.3" name="section-4.2.3">4.2.3</a>.  Text in Control Channel</span>

   The SPEECHSC framework assumes all TTS servers accept text over the
   SPEECHSC connection for reading over the RTP connection.  The
   framework assumes the server can accept text either "by value"
   (embedded in the protocol) or "by reference" (e.g., by de-referencing
   a Uniform Resource Identifier (URI) embedded in the protocol).

<span class="h4"><a class="dashAnchor" name="//apple_ref/Section/4.2.4.%20%20Document%20Type%20Indication"></a><a class="selflink" href="#section-4.2.4" name="section-4.2.4">4.2.4</a>.  Document Type Indication</span>

   A document type specifies the syntax in which the text to be read is
   encoded.  The SPEECHSC framework MUST be capable of explicitly
   indicating the document type of the text to be processed, as opposed
   to forcing the server to infer the content by other means.

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/4.3.%20%20Control%20Channel"></a><a class="selflink" href="#section-4.3" name="section-4.3">4.3</a>.  Control Channel</span>

   The SPEECHSC framework MUST be capable of establishing the control
   channel between the client and server on a per-session basis, where a
   session is loosely defined to be associated with a single "call" or
   "dialog".  The protocol SHOULD be capable of maintaining a long-lived
   control channel for multiple sessions serially, and MAY be capable of
   shorter time horizons as well, including as short as for the
   processing of a single utterance.

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/4.4.%20%20Media%20Origination%2FTermination%20by%20Control%20Elements"></a><a class="selflink" href="#section-4.4" name="section-4.4">4.4</a>.  Media Origination/Termination by Control Elements</span>

   The SPEECHSC framework MUST NOT require the controlling element
   (application server, media processing entity) to accept or originate
   media streams.  Media streams MAY source &amp; sink from the controlled
   element (ASR, TTS, etc.).

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/4.5.%20%20Playback%20Controls"></a><a class="selflink" href="#section-4.5" name="section-4.5">4.5</a>.  Playback Controls</span>

   The SPEECHSC framework MUST support "VCR controls" for controlling
   the playout of streaming media output from SPEECHSC processing, and
   MUST allow for servers with varying capabilities to accommodate such
   controls.  The protocol SHOULD allow clients to state what controls
   they wish to use, and for servers to report which ones they honor.
   These capabilities include:







<span class="grey">Oran                         Informational                     [Page 10]</span></pre>
<hr align="left" class="noprint" style="width: 96ex;"/><!--NewPage--><pre class="newpage"><a class="invisible" href="#page-11" id="page-11" name="page-11"> </a>
<span class="grey"><a href="rfc4313.html">RFC 4313</a>          Speech Services Control Requirements     December 2005</span>


   o  The ability to jump in time to the location of a specific marker.
   o  The ability to jump in time, forwards or backwards, by a specified
      amount of time.  Valid time units MUST include seconds, words,
      paragraphs, sentences, and markers.
   o  The ability to increase and decrease playout speed.
   o  The ability to fast-forward and fast-rewind the audio, where
      snippets of audio are played as the server moves forwards or
      backwards in time.
   o  The ability to pause and resume playout.
   o  The ability to increase and decrease playout volume.

   These controls SHOULD be made easily available to users through the
   client user interface and through per-user customization capabilities
   of the client.  This is particularly important for hearing-impaired
   users, who will likely desire settings and control regimes different
   from those that would be acceptable for non-impaired users.

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/4.6.%20%20Session%20Parameters"></a><a class="selflink" href="#section-4.6" name="section-4.6">4.6</a>.  Session Parameters</span>

   The SPEECHSC framework MUST support the specification of session
   parameters, such as language, prosody, and voicing.

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/4.7.%20%20Speech%20Markers"></a><a class="selflink" href="#section-4.7" name="section-4.7">4.7</a>.  Speech Markers</span>

   The SPEECHSC framework MUST accommodate speech markers, with
   capability at least as flexible as that provided in SSML [<a href="#ref-1" title='"Speech Synthesis Markup Language (SSML) Version 1.0"'>1</a>].  The
   framework MUST further provide an efficient mechanism for reporting
   that a marker has been reached during playout.

<span class="h2"><a class="dashAnchor" name="//apple_ref/Section/5.%20%20ASR%20Requirements"></a><a class="selflink" href="#section-5" name="section-5">5</a>.  ASR Requirements</span>

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/5.1.%20%20Requesting%20Automatic%20Speech%20Recognition"></a><a class="selflink" href="#section-5.1" name="section-5.1">5.1</a>.  Requesting Automatic Speech Recognition</span>

   The SPEECHSC framework MUST allow a Media Processing Entity or
   Application Server to request the ASR Server to perform automatic
   speech recognition on an RTP stream, returning the results over
   SPEECHSC.

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/5.2.%20%20XML"></a><a class="selflink" href="#section-5.2" name="section-5.2">5.2</a>.  XML</span>

   The SPEECHSC framework assumes that all ASR servers support the
   VoiceXML speech recognition grammar specification (SRGS) for speech
   recognition [<a href="#ref-2" title='"Speech Recognition Grammar Specification Version 1.0"'>2</a>].








<span class="grey">Oran                         Informational                     [Page 11]</span></pre>
<hr align="left" class="noprint" style="width: 96ex;"/><!--NewPage--><pre class="newpage"><a class="invisible" href="#page-12" id="page-12" name="page-12"> </a>
<span class="grey"><a href="rfc4313.html">RFC 4313</a>          Speech Services Control Requirements     December 2005</span>


<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/5.3.%20%20Grammar%20Requirements"></a><a class="selflink" href="#section-5.3" name="section-5.3">5.3</a>.  Grammar Requirements</span>

<span class="h4"><a class="dashAnchor" name="//apple_ref/Section/5.3.1.%20%20Grammar%20Specification"></a><a class="selflink" href="#section-5.3.1" name="section-5.3.1">5.3.1</a>.  Grammar Specification</span>

   The SPEECHSC framework assumes all ASR servers are capable of
   accepting grammar specifications either "by value" (embedded in the
   protocol) or "by reference" (e.g., by de-referencing a URI embedded
   in the protocol).  The latter MUST allow the indication of a grammar
   already known to, or otherwise "built in" to, the server.  The
   framework and protocol further SHOULD exploit the ability to store
   and later retrieve by reference large grammars that were originally
   supplied by the client.

<span class="h4"><a class="dashAnchor" name="//apple_ref/Section/5.3.2.%20%20Explicit%20Indication%20of%20Grammar%20Format"></a><a class="selflink" href="#section-5.3.2" name="section-5.3.2">5.3.2</a>.  Explicit Indication of Grammar Format</span>

   The SPEECHSC framework protocol MUST be able to explicitly convey the
   grammar format in which the grammar is encoded and MUST be extensible
   to allow for conveying new grammar formats as they are defined.

<span class="h4"><a class="dashAnchor" name="//apple_ref/Section/5.3.3.%20%20Grammar%20Sharing"></a><a class="selflink" href="#section-5.3.3" name="section-5.3.3">5.3.3</a>.  Grammar Sharing</span>

   The SPEECHSC framework SHOULD exploit sharing grammars across
   sessions for servers that are capable of doing so.  This supports
   applications with large grammars for which it is unrealistic to
   dynamically load.  An example is a city-country grammar for a weather
   service.

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/5.4.%20%20Session%20Parameters"></a><a class="selflink" href="#section-5.4" name="section-5.4">5.4</a>.  Session Parameters</span>

   The SPEECHSC framework MUST accommodate at a minimum all of the
   protocol parameters currently defined in Media Resource Control
   Protocol (MRCP) [<a href="#ref-10" title='"MRCP: Media Resource Control Protocol"'>10</a>] In addition, there SHOULD be a capability to
   reset parameters within a session.

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/5.5.%20%20Input%20Capture"></a><a class="selflink" href="#section-5.5" name="section-5.5">5.5</a>.  Input Capture</span>

   The SPEECHSC framework MUST support a method directing the ASR Server
   to capture the input media stream for later analysis and tuning of
   the ASR engine.












<span class="grey">Oran                         Informational                     [Page 12]</span></pre>
<hr align="left" class="noprint" style="width: 96ex;"/><!--NewPage--><pre class="newpage"><a class="invisible" href="#page-13" id="page-13" name="page-13"> </a>
<span class="grey"><a href="rfc4313.html">RFC 4313</a>          Speech Services Control Requirements     December 2005</span>


<span class="h2"><a class="dashAnchor" name="//apple_ref/Section/6.%20%20Speaker%20Identification%20and%20Verification%20Requirements"></a><a class="selflink" href="#section-6" name="section-6">6</a>.  Speaker Identification and Verification Requirements</span>

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/6.1.%20%20Requesting%20SI%2FSV"></a><a class="selflink" href="#section-6.1" name="section-6.1">6.1</a>.  Requesting SI/SV</span>

   The SPEECHSC framework MUST allow a Media Processing Entity to
   request the SI/SV Server to perform speaker identification or
   verification on an RTP stream, returning the results over SPEECHSC.

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/6.2.%20%20Identifiers%20for%20SI%2FSV"></a><a class="selflink" href="#section-6.2" name="section-6.2">6.2</a>.  Identifiers for SI/SV</span>

   The SPEECHSC framework MUST accommodate an identifier for each
   verification resource and permit control of that resource by ID,
   because voiceprint format and contents are vendor specific.

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/6.3.%20%20State%20for%20Multiple%20Utterances"></a><a class="selflink" href="#section-6.3" name="section-6.3">6.3</a>.  State for Multiple Utterances</span>

   The SPEECHSC framework MUST work with SI/SV servers that maintain
   state to handle multi-utterance verification.

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/6.4.%20%20Input%20Capture"></a><a class="selflink" href="#section-6.4" name="section-6.4">6.4</a>.  Input Capture</span>

   The SPEECHSC framework MUST support a method for capturing the input
   media stream for later analysis and tuning of the SI/SV engine.  The
   framework may assume all servers are capable of doing so.  In
   addition, the framework assumes that the captured stream contains
   enough timestamp context (e.g., the NTP time range from the RTP
   Control Protocol (RTCP) packets, which corresponds to the RTP
   timestamps of the captured input) to ascertain after the fact exactly
   when the verification was requested.

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/6.5.%20%20SI%2FSV%20Functional%20Extensibility"></a><a class="selflink" href="#section-6.5" name="section-6.5">6.5</a>.  SI/SV Functional Extensibility</span>

   The SPEECHSC framework SHOULD be extensible to additional functions
   associated with SI/SV, such as prompting, utterance verification, and
   retraining.

<span class="h2"><a class="dashAnchor" name="//apple_ref/Section/7.%20%20Duplexing%20and%20Parallel%20Operation%20Requirements"></a><a class="selflink" href="#section-7" name="section-7">7</a>.  Duplexing and Parallel Operation Requirements</span>

   One very important requirement for an interactive speech-driven
   system is that user perception of the quality of the interaction
   depends strongly on the ability of the user to interrupt a prompt or
   rendered TTS with speech.  Interrupting, or barging, the speech
   output requires more than energy detection from the user's direction.
   Many advanced systems halt the media towards the user by employing
   the ASR engine to decide if an utterance is likely to be real speech,
   as opposed to a cough, for example.





<span class="grey">Oran                         Informational                     [Page 13]</span></pre>
<hr align="left" class="noprint" style="width: 96ex;"/><!--NewPage--><pre class="newpage"><a class="invisible" href="#page-14" id="page-14" name="page-14"> </a>
<span class="grey"><a href="rfc4313.html">RFC 4313</a>          Speech Services Control Requirements     December 2005</span>


<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/7.1.%20%20Full%20Duplex%20Operation"></a><a class="selflink" href="#section-7.1" name="section-7.1">7.1</a>.  Full Duplex Operation</span>

   To achieve low latency between utterance detection and halting of
   playback, many implementations combine the speaking and ASR
   functions.  The SPEECHSC framework MUST support such full-duplex
   implementations.

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/7.2.%20%20Multiple%20Services%20in%20Parallel"></a><a class="selflink" href="#section-7.2" name="section-7.2">7.2</a>.  Multiple Services in Parallel</span>

   Good spoken user interfaces typically depend upon the ease with which
   the user can accomplish his or her task.  When making use of speaker
   identification or verification technologies, user interface
   improvements often come from the combination of the different
   technologies: simultaneous identity claim and verification (on the
   same utterance), simultaneous knowledge and voice verification (using
   ASR and verification simultaneously).  Using ASR and verification on
   the same utterance is in fact the only way to support rolling or
   dynamically-generated challenge phrases (e.g., "say 51723").  The
   SPEECHSC framework MUST support such parallel service
   implementations.

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/7.3.%20%20Combination%20of%20Services"></a><a class="selflink" href="#section-7.3" name="section-7.3">7.3</a>.  Combination of Services</span>

   It is optionally of interest that the SPEECHSC framework support more
   complex remote combination and controls of speech engines:

   o  Combination in series of engines that may then act on the input or
      output of ASR, TTS, or Speaker recognition engines.  The control
      MAY then extend beyond such engines to include other audio input
      and output processing and natural language processing.
   o  Intermediate exchanges and coordination between engines.
   o  Remote specification of flows between engines.

   These capabilities MAY benefit from service discovery mechanisms
   (e.g., engines, properties, and states discovery).

<span class="h2"><a class="dashAnchor" name="//apple_ref/Section/8.%20%20Additional%20Considerations%20%28Non-Normative%29"></a><a class="selflink" href="#section-8" name="section-8">8</a>.  Additional Considerations (Non-Normative)</span>

   The framework assumes that Session Description Protocol (SDP) will be
   used to describe media sessions and streams.  The framework further
   assumes RTP carriage of media.  However, since SDP can be used to
   describe other media transport schemes (e.g., ATM) these could be
   used if they provide the necessary elements (e.g., explicit
   timestamps).







<span class="grey">Oran                         Informational                     [Page 14]</span></pre>
<hr align="left" class="noprint" style="width: 96ex;"/><!--NewPage--><pre class="newpage"><a class="invisible" href="#page-15" id="page-15" name="page-15"> </a>
<span class="grey"><a href="rfc4313.html">RFC 4313</a>          Speech Services Control Requirements     December 2005</span>


   The working group will not be defining distributed speech recognition
   (DSR) methods, as exemplified by the European Telecommunications
   Standards Institute (ETSI) Aurora project.  The working group will
   not be recreating functionality available in other protocols, such as
   SIP or SDP.

   TTS looks very much like playing back a file.  Extending RTSP looks
   promising for when one requires VCR controls or markers in the text
   to be spoken.  When one does not require VCR controls, SIP in a
   framework such as Network Announcements [<a href="#ref-12" title='"Basic Network Media Services with SIP"'>12</a>] works directly without
   modification.

   ASR has an entirely different set of characteristics.  For barge-in
   support, ASR requires real-time return of intermediate results.
   Barring the discovery of a good reuse model for an existing protocol,
   this will most likely become the focus of SPEECHSC.

<span class="h2"><a class="dashAnchor" name="//apple_ref/Section/9.%20%20Security%20Considerations"></a><a class="selflink" href="#section-9" name="section-9">9</a>.  Security Considerations</span>

   Protocols relating to speech processing must take security and
   privacy into account.  Many applications of speech technology deal
   with sensitive information, such as the use of Text-to-Speech to read
   financial information.  Likewise, popular uses for automatic speech
   recognition include executing financial transactions and shopping.

   There are at least three aspects of speech processing security that
   intersect with the SPEECHSC requirements -- securing the SPEECHSC
   protocol itself, implementing and deploying the servers that run the
   protocol, and ensuring that utilization of the technology for
   providing security functions is appropriate.  Each of these aspects
   in discussed in the following subsections.  While some of these
   considerations are, strictly speaking, out of scope of the protocol
   itself, they will be carefully considered and accommodated during
   protocol design, and will be called out as part of the applicability
   statement accompanying the protocol specification(s).  Privacy
   considerations are discussed as well.

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/9.1.%20%20SPEECHSC%20Protocol%20Security"></a><a class="selflink" href="#section-9.1" name="section-9.1">9.1</a>.  SPEECHSC Protocol Security</span>

   The SPEECHSC protocol MUST in all cases support authentication,
   authorization, and integrity, and SHOULD support confidentiality.
   For privacy-sensitive applications, the protocol MUST support
   confidentiality.  We envision that rather than providing
   protocol-specific security mechanisms in SPEECHSC itself, the
   resulting protocol will employ security machinery of either a
   containing protocol or the transport on which it runs.  For example,
   we will consider solutions such as using Transport Layer Security
   (TLS) for securing the control channel, and Secure Realtime Transport



<span class="grey">Oran                         Informational                     [Page 15]</span></pre>
<hr align="left" class="noprint" style="width: 96ex;"/><!--NewPage--><pre class="newpage"><a class="invisible" href="#page-16" id="page-16" name="page-16"> </a>
<span class="grey"><a href="rfc4313.html">RFC 4313</a>          Speech Services Control Requirements     December 2005</span>


   Protocol (SRTP) for securing the media channel.  Third-party
   dependencies necessitating transitive trust will be minimized or
   explicitly dealt with through the authentication and authorization
   aspects of the protocol design.

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/9.2.%20%20Client%20and%20Server%20Implementation%20and%20Deployment"></a><a class="selflink" href="#section-9.2" name="section-9.2">9.2</a>.  Client and Server Implementation and Deployment</span>

   Given the possibly sensitive nature of the information carried,
   SPEECHSC clients and servers need to take steps to ensure
   confidentiality and integrity of the data and its transformations to
   and from spoken form.  In addition to these general considerations,
   certain SPEECHSC functions, such as speaker verification and
   identification, employ voiceprints whose privacy, confidentiality,
   and integrity must be maintained.  Similarly, the requirement to
   support input capture for analysis and tuning can represent a privacy
   vulnerability because user utterances are recorded and could be
   either revealed or replayed inappropriately.  Implementers must take
   care to prevent the exploitation of any centralized voiceprint
   database and the recorded material from which such voiceprints may be
   derived.  Specific actions that are recommended to minimize these
   threats include:

   o  End-to-end authentication, confidentiality, and integrity
      protection (like TLS) of access to the database to minimize the
      exposure to external attack.
   o  Database protection measures such as read/write access control and
      local login authentication to minimize the exposure to insider
      threats.
   o  Copies of the database, especially ones that are maintained at
      off-site locations, need the same protection as the operational
      database.

   Inappropriate disclosure of this data does not as of the date of this
   document represent an exploitable threat, but quite possibly might in
   the future.  Specific vulnerabilities that might become feasible are
   discussed in the next subsection.  It is prudent to take measures
   such as encrypting the voiceprint database and permitting access only
   through programming interfaces enforcing adequate authorization
   machinery.

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/9.3.%20%20Use%20of%20SPEECHSC%20for%20Security%20Functions"></a><a class="selflink" href="#section-9.3" name="section-9.3">9.3</a>.  Use of SPEECHSC for Security Functions</span>

   Either speaker identification or verification can be used directly as
   an authentication technology.  Authorization decisions can be coupled
   with speaker verification in a direct fashion through
   challenge-response protocols, or indirectly with speaker
   identification through the use of access control lists or other
   identity-based authorization mechanisms.  When so employed, there are



<span class="grey">Oran                         Informational                     [Page 16]</span></pre>
<hr align="left" class="noprint" style="width: 96ex;"/><!--NewPage--><pre class="newpage"><a class="invisible" href="#page-17" id="page-17" name="page-17"> </a>
<span class="grey"><a href="rfc4313.html">RFC 4313</a>          Speech Services Control Requirements     December 2005</span>


   additional security concerns that need to be addressed through the
   use of protocol security mechanisms for clients and servers.  For
   example, the ability to manipulate the media stream of a speaker
   verification request could inappropriately permit or deny access
   based on impersonation, or simple garbling via noise injection,
   making it critical to properly secure both the control and data
   channels, as recommended above.  The following issues specific to the
   use of SI/SV for authentication should be carefully considered:

   1.  Theft of voiceprints or the recorded samples used to construct
       them represents a future threat against the use of speaker
       identification/verification as a biometric authentication
       technology.  A plausible attack vector (not feasible today) is to
       use the voiceprint information as parametric input to a
       text-to-speech synthesis system that could mimic the user's voice
       accurately enough to match the voiceprint.  Since it is not very
       difficult to surreptitiously record reasonably large corpuses of
       voice samples, the ability to construct voiceprints for input to
       this attack would render the security of voice-based biometric
       authentication, even using advanced challenge-response
       techniques, highly vulnerable.  Users of speaker verification for
       authentication should monitor technological developments in this
       area closely for such future vulnerabilities (much as users of
       other authentication technologies should monitor advances in
       factoring as a way to break asymmetric keying systems).
   2.  As with other biometric authentication technologies, a downside
       to the use of speech identification is that revocation is not
       possible.  Once compromised, the biometric information can be
       used in identification and authentication to other independent
       systems.
   3.  Enrollment procedures can be vulnerable to impersonation if not
       protected both by protocol security mechanisms and some
       independent proof of identity.  (Proof of identity may not be
       needed in systems that only need to verify continuity of identity
       since enrollment, as opposed to association with a particular
       individual.

   Further discussion of the use of SI/SV as an authentication
   technology, and some recommendations concerning advantages and
   vulnerabilities, can be found in Chapter 5 of [<a href="#ref-15" title='"Who Goes There?: Authentication Through the Lens of Privacy"'>15</a>].

<span class="h2"><a class="dashAnchor" name="//apple_ref/Section/10.%20%20Acknowledgements"></a><a class="selflink" href="#section-10" name="section-10">10</a>.  Acknowledgements</span>

   Eric Burger wrote the original version of these requirements and has
   continued to contribute actively throughout their development.  He is
   a co-author in all but formal authorship, and is instead acknowledged
   here as it is preferable that working group co-chairs have
   non-conflicting roles with respect to the progression of documents.



<span class="grey">Oran                         Informational                     [Page 17]</span></pre>
<hr align="left" class="noprint" style="width: 96ex;"/><!--NewPage--><pre class="newpage"><a class="invisible" href="#page-18" id="page-18" name="page-18"> </a>
<span class="grey"><a href="rfc4313.html">RFC 4313</a>          Speech Services Control Requirements     December 2005</span>


<span class="h2"><a class="dashAnchor" name="//apple_ref/Section/11.%20%20References"></a><a class="selflink" href="#section-11" name="section-11">11</a>.  References</span>

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/11.1.%20%20Normative%20References"></a><a class="selflink" href="#section-11.1" name="section-11.1">11.1</a>.  Normative References</span>

   [<a id="ref-1" name="ref-1">1</a>]  Walker, M., Burnett, D., and A. Hunt, "Speech Synthesis Markup
        Language (SSML) Version 1.0", W3C
        REC REC-speech-synthesis-20040907, September 2004.

   [<a id="ref-2" name="ref-2">2</a>]  McGlashan, S. and A. Hunt, "Speech Recognition Grammar
        Specification Version 1.0", W3C REC REC-speech-grammar-20040316,
        March 2004.

   [<a id="ref-3" name="ref-3">3</a>]  Bradner, S., "Key words for use in RFCs to Indicate Requirement
        Levels", <a href="https://tools.ietf.org/html/bcp14">BCP 14</a>, <a href="rfc2119.html">RFC 2119</a>, March 1997.

   [<a id="ref-4" name="ref-4">4</a>]  Floyd, S. and L. Daigle, "IAB Architectural and Policy
        Considerations for Open Pluggable Edge Services", <a href="rfc3238.html">RFC 3238</a>,
        January 2002.

   [<a id="ref-5" name="ref-5">5</a>]  Charlton, N., Gasson, M., Gybels, G., Spanner, M., and A. van
        Wijk, "User Requirements for the Session Initiation Protocol
        (SIP) in Support of Deaf, Hard of Hearing and Speech-impaired
        Individuals", <a href="rfc3351.html">RFC 3351</a>, August 2002.

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/11.2.%20%20Informative%20References"></a><a class="selflink" href="#section-11.2" name="section-11.2">11.2</a>.  Informative References</span>

   [<a id="ref-6" name="ref-6">6</a>]   Rosenberg, J., Schulzrinne, H., Camarillo, G., Johnston, A.,
         Peterson, J., Sparks, R., Handley, M., and E. Schooler, "SIP:
         Session Initiation Protocol", <a href="rfc3261.html">RFC 3261</a>, June 2002.

   [<a id="ref-7" name="ref-7">7</a>]   Andreasen, F. and B. Foster, "Media Gateway Control Protocol
         (MGCP) Version 1.0", <a href="rfc3435.html">RFC 3435</a>, January 2003.

   [<a id="ref-8" name="ref-8">8</a>]   Groves, C., Pantaleo, M., Ericsson, LM., Anderson, T., and T.
         Taylor, "Gateway Control Protocol Version 1", <a href="rfc3525.html">RFC 3525</a>,
         June 2003.

   [<a id="ref-9" name="ref-9">9</a>]   Schulzrinne, H., Rao, A., and R. Lanphier, "Real Time Streaming
         Protocol (RTSP)", <a href="rfc2326.html">RFC 2326</a>, April 1998.

   [<a id="ref-10" name="ref-10">10</a>]  Shanmugham, S., Monaco, P., and B. Eberman, "MRCP: Media
         Resource Control Protocol", Work in Progress.









<span class="grey">Oran                         Informational                     [Page 18]</span></pre>
<hr align="left" class="noprint" style="width: 96ex;"/><!--NewPage--><pre class="newpage"><a class="invisible" href="#page-19" id="page-19" name="page-19"> </a>
<span class="grey"><a href="rfc4313.html">RFC 4313</a>          Speech Services Control Requirements     December 2005</span>


   [<a id="ref-11" name="ref-11">11</a>]  World Wide Web Consortium, "Voice Extensible Markup Language
         (VoiceXML) Version 2.0", W3C Working Draft , April 2002,
         &lt;<a href="http://www.w3.org/TR/2002/WD-voicexml20-20020424/">http://www.w3.org/TR/2002/WD-voicexml20-20020424/</a>&gt;.

   [<a id="ref-12" name="ref-12">12</a>]  Burger, E., Ed., Van Dyke, J., and A. Spitzer, "Basic Network
         Media Services with SIP", <a href="rfc4240.html">RFC 4240</a>, December 2005.

   [<a id="ref-13" name="ref-13">13</a>]  Guttman, E., Perkins, C., Veizades, J., and M. Day, "Service
         Location Protocol, Version 2", <a href="rfc2608.html">RFC 2608</a>, June 1999.

   [<a id="ref-14" name="ref-14">14</a>]  Gulbrandsen, A., Vixie, P., and L. Esibov, "A DNS RR for
         specifying the location of services (DNS SRV)", <a href="rfc2782.html">RFC 2782</a>,
         February 2000.

   [<a id="ref-15" name="ref-15">15</a>]  Committee on Authentication Technologies and Their Privacy
         Implications, National Research Council, "Who Goes There?:
         Authentication Through the Lens of Privacy", Computer Science
         and Telecommunications Board (CSTB) , 2003,
         &lt;<a href="http://www.nap.edu/catalog/10656.html/">http://www.nap.edu/catalog/10656.html/</a> &gt;.

Author's Address

   David R. Oran
   Cisco Systems, Inc.
   7 Ladyslipper Lane
   Acton, MA
   USA

   EMail: oran@cisco.com






















<span class="grey">Oran                         Informational                     [Page 19]</span></pre>
<hr align="left" class="noprint" style="width: 96ex;"/><!--NewPage--><pre class="newpage"><a class="invisible" href="#page-20" id="page-20" name="page-20"> </a>
<span class="grey"><a href="rfc4313.html">RFC 4313</a>          Speech Services Control Requirements     December 2005</span>


Full Copyright Statement

   Copyright (C) The Internet Society (2005).

   This document is subject to the rights, licenses and restrictions
   contained in <a href="https://tools.ietf.org/html/bcp78">BCP 78</a>, and except as set forth therein, the authors
   retain all their rights.

   This document and the information contained herein are provided on an
   "AS IS" basis and THE CONTRIBUTOR, THE ORGANIZATION HE/SHE REPRESENTS
   OR IS SPONSORED BY (IF ANY), THE INTERNET SOCIETY AND THE INTERNET
   ENGINEERING TASK FORCE DISCLAIM ALL WARRANTIES, EXPRESS OR IMPLIED,
   INCLUDING BUT NOT LIMITED TO ANY WARRANTY THAT THE USE OF THE
   INFORMATION HEREIN WILL NOT INFRINGE ANY RIGHTS OR ANY IMPLIED
   WARRANTIES OF MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE.

Intellectual Property

   The IETF takes no position regarding the validity or scope of any
   Intellectual Property Rights or other rights that might be claimed to
   pertain to the implementation or use of the technology described in
   this document or the extent to which any license under such rights
   might or might not be available; nor does it represent that it has
   made any independent effort to identify any such rights.  Information
   on the procedures with respect to rights in RFC documents can be
   found in <a href="https://tools.ietf.org/html/bcp78">BCP 78</a> and <a href="https://tools.ietf.org/html/bcp79">BCP 79</a>.

   Copies of IPR disclosures made to the IETF Secretariat and any
   assurances of licenses to be made available, or the result of an
   attempt made to obtain a general license or permission for the use of
   such proprietary rights by implementers or users of this
   specification can be obtained from the IETF on-line IPR repository at
   <a href="http://www.ietf.org/ipr">http://www.ietf.org/ipr</a>.

   The IETF invites any interested party to bring to its attention any
   copyrights, patents or patent applications, or other proprietary
   rights that may cover technology that may be required to implement
   this standard.  Please address the information to the IETF at ietf-
   ipr@ietf.org.

Acknowledgement

   Funding for the RFC Editor function is currently provided by the
   Internet Society.







Oran                         Informational                     [Page 20]

</pre><br/>
    <span class="noprint"><small><small>Html markup produced by rfcmarkup 1.126, available from
      <a href="https://tools.ietf.org/tools/rfcmarkup/">https://tools.ietf.org/tools/rfcmarkup/</a>
    </small></small></span>
  </div>




</body><!-- Mirrored from tools.ietf.org/html/rfc4313 by HTTrack Website Copier/3.x [XR&CO'2014], Tue, 06 Mar 2018 23:18:41 GMT --></html>