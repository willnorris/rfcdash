<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml"><!-- Mirrored from tools.ietf.org/html/rfc2201 by HTTrack Website Copier/3.x [XR&CO'2014], Tue, 06 Mar 2018 23:18:54 GMT --><!-- Added by HTTrack --><head><meta content="text/html;charset=utf-8" http-equiv="content-type"/><!-- /Added by HTTrack -->

    <meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
    <meta content="index,follow" name="robots"/>
    <meta content="rfcmarkup version 1.126" name="creator"/>
    <link href="http://purl.org/dc/elements/1.1/" rel="schema.DC"/>
<meta content="urn:ietf:rfc:2201" name="DC.Identifier"/>
<meta content="CBT is a multicast routing architecture that builds a single delivery
tree per group which is shared by all of the group's senders and
receivers. This memo defines an Experimental Protocol for the Internet
community." name="DC.Description.Abstract"/>
<meta content="A. Ballardie" name="DC.Creator"/>
<meta content="September, 1997" name="DC.Date.Issued"/>
<meta content="Core Based Trees (CBT) Multicast Routing Architecture" name="DC.Title"/>

    <link href="https://tools.ietf.org/images/rfc.png" rel="icon" type="image/png"/>
    <link href="https://tools.ietf.org/images/rfc.png" rel="shortcut icon" type="image/png"/>
    <title>RFC 2201 - Core Based Trees (CBT) Multicast Routing Architecture</title>
    
    
    <style type="text/css">
	@media only screen 
	  and (min-width: 992px)
	  and (max-width: 1199px) {
	    body { font-size: 14pt; }
            div.content { width: 96ex; margin: 0 auto; }
        }
	@media only screen 
	  and (min-width: 768px)
	  and (max-width: 991px) {
            body { font-size: 14pt; }
            div.content { width: 96ex; margin: 0 auto; }
        }
	@media only screen 
	  and (min-width: 480px)
	  and (max-width: 767px) {
            body { font-size: 11pt; }
            div.content { width: 96ex; margin: 0 auto; }
        }
	@media only screen 
	  and (max-width: 479px) {
            body { font-size: 8pt; }
            div.content { width: 96ex; margin: 0 auto; }
        }
	@media only screen 
	  and (min-device-width : 375px) 
	  and (max-device-width : 667px) {
            body { font-size: 9.5pt; }
            div.content { width: 96ex; margin: 0 1px; }
        }
	@media only screen 
	  and (min-device-width: 1200px) {
            body { font-size: 10pt; margin: 0 4em; }
            div.content { width: 96ex; margin: 0; }
        }
        h1, h2, h3, h4, h5, h6, .h1, .h2, .h3, .h4, .h5, .h6 {
	    font-weight: bold;
            line-height: 0pt;
            display: inline;
            white-space: pre;
            font-family: monospace;
            font-size: 1em;
	    font-weight: bold;
        }
        pre {
            font-size: 1em;
            margin-top: 0px;
            margin-bottom: 0px;
        }
	.pre {
	    white-space: pre;
	    font-family: monospace;
	}
	.header{
	    font-weight: bold;
	}
        .newpage {
            page-break-before: always;
        }
        .invisible {
            text-decoration: none;
            color: white;
        }
        a.selflink {
          color: black;
          text-decoration: none;
        }
        @media print {
            body {
                font-family: monospace;
                font-size: 10.5pt;
            }
            h1, h2, h3, h4, h5, h6 {
                font-size: 1em;
            }
        
            a:link, a:visited {
                color: inherit;
                text-decoration: none;
            }
            .noprint {
                display: none;
            }
        }
	@media screen {
	    .grey, .grey a:link, .grey a:visited {
		color: #777;
	    }
            .docinfo {
                background-color: #EEE;
            }
            .top {
                border-top: 7px solid #EEE;
            }
            .bgwhite  { background-color: white; }
            .bgred    { background-color: #F44; }
            .bggrey   { background-color: #666; }
            .bgbrown  { background-color: #840; }            
            .bgorange { background-color: #FA0; }
            .bgyellow { background-color: #EE0; }
            .bgmagenta{ background-color: #F4F; }
            .bgblue   { background-color: #66F; }
            .bgcyan   { background-color: #4DD; }
            .bggreen  { background-color: #4F4; }

            .legend   { font-size: 90%; }
            .cplate   { font-size: 70%; border: solid grey 1px; }
	}
    </style>
    <!--[if IE]>
    <style>
    body {
       font-size: 13px;
       margin: 10px 10px;
    }
    </style>
    <![endif]-->

    <script type="text/javascript"><!--
    function addHeaderTags() {
	var spans = document.getElementsByTagName("span");
	for (var i=0; i < spans.length; i++) {
	    var elem = spans[i];
	    if (elem) {
		var level = elem.getAttribute("class");
                if (level == "h1" || level == "h2" || level == "h3" || level == "h4" || level == "h5" || level == "h6") {
                    elem.innerHTML = "<"+level+">"+elem.innerHTML+"</"+level+">";		
                }
	    }
	}
    }
    var legend_html = "Colour legend:<br />      <table>         <tr><td>Unknown:</td>                   <td><span class='cplate bgwhite'>&nbsp;&nbsp;&nbsp;&nbsp;</span></td></tr>         <tr><td>Draft:</td>                     <td><span class='cplate bgred'>&nbsp;&nbsp;&nbsp;&nbsp;</span></td></tr>         <tr><td>Informational:</td>             <td><span class='cplate bgorange'>&nbsp;&nbsp;&nbsp;&nbsp;</span></td></tr>         <tr><td>Experimental:</td>              <td><span class='cplate bgyellow'>&nbsp;&nbsp;&nbsp;&nbsp;</span></td></tr>         <tr><td>Best Common Practice:</td>      <td><span class='cplate bgmagenta'>&nbsp;&nbsp;&nbsp;&nbsp;</span></td></tr>         <tr><td>Proposed Standard:</td>         <td><span class='cplate bgblue'>&nbsp;&nbsp;&nbsp;&nbsp;</span></td></tr>         <tr><td>Draft Standard (old designation):</td> <td><span class='cplate bgcyan'>&nbsp;&nbsp;&nbsp;&nbsp;</span></td></tr>         <tr><td>Internet Standard:</td>         <td><span class='cplate bggreen'>&nbsp;&nbsp;&nbsp;&nbsp;</span></td></tr>         <tr><td>Historic:</td>                  <td><span class='cplate bggrey'>&nbsp;&nbsp;&nbsp;&nbsp;</span></td></tr>         <tr><td>Obsolete:</td>                  <td><span class='cplate bgbrown'>&nbsp;&nbsp;&nbsp;&nbsp;</span></td></tr>     </table>";
    function showElem(id) {
        var elem = document.getElementById(id);
        elem.innerHTML = eval(id+"_html");
        elem.style.visibility='visible';
    }
    function hideElem(id) {
        var elem = document.getElementById(id);
        elem.style.visibility='hidden';        
        elem.innerHTML = "";
    }
    // -->
    </script>
</head>
<body onload="addHeaderTags()">
  <div class="content">
   <div style="height: 13px;">
      <div class="pre noprint docinfo bggrey" onclick="showElem('legend');" onmouseout="hideElem('legend')" onmouseover="this.style.cursor='pointer';" style="height: 6px; position: absolute;" title="Click for colour legend.">                                                                        </div>
      <div class="docinfo noprint pre legend" id="legend" onmouseout="hideElem('legend');" onmouseover="showElem('legend');" style="position:absolute; top: 4px; left: 4ex; visibility:hidden; background-color: white; padding: 4px 9px 5px 7px; border: solid #345 1px; ">
      </div>
   </div>
<span class="pre noprint docinfo top">[<a href="index.html" title="Document search and retrieval page">Docs</a>] [<a href="https://tools.ietf.org/rfc/rfc2201.txt" title="Plaintext version of this document">txt</a>|<a href="https://tools.ietf.org/pdf/rfc2201" title="PDF version of this document">pdf</a>] [<a href="https://tools.ietf.org/html/draft-ietf-idmr-cbt-arch" title="draft-ietf-idmr-cbt-arch">draft-ietf-idmr...</a>] [<a href="https://datatracker.ietf.org/doc/rfc2201" title="IESG Datatracker information for this document">Tracker</a>] [<a href="https://tools.ietf.org/rfcdiff?difftype=--hwdiff&amp;url2=rfc2201" title="Inline diff (wdiff)">Diff1</a>] [<a href="https://tools.ietf.org/rfcdiff?url2=rfc2201" title="Side-by-side diff">Diff2</a>]         </span><br/>
<span class="pre noprint docinfo">                                                                        </span><br/>
<span class="pre noprint docinfo">                                                                HISTORIC</span><br/>
<span class="pre noprint docinfo">                                                                        </span><br/>
<pre>Network Working Group                                       A. Ballardie
Request for Comments: 2201                                    Consultant
Category: Experimental                                    September 1997


         <span class="h1">Core Based Trees (CBT) Multicast Routing Architecture</span>

Status of this Memo

   This memo defines an Experimental Protocol for the Internet
   community.  This memo does not specify an Internet standard of any
   kind.  Discussion and suggestions for improvement are requested.
   Distribution of this memo is unlimited.

Abstract

   CBT is a multicast routing architecture that builds a single delivery
   tree per group which is shared by all of the group's senders and
   receivers.  Most multicast algorithms build one multicast tree per
   sender (subnetwork), the tree being rooted at the sender's
   subnetwork.  The primary advantage of the shared tree approach is
   that it typically offers more favourable scaling characteristics than
   all other multicast algorithms.

   The CBT protocol [<a href="#ref-1" title='"Core Based Trees (CBT version 2) Multicast Routing: Protocol Specification"'>1</a>] is a network layer multicast routing protocol
   that builds and maintains a shared delivery tree for a multicast
   group.  The sending and receiving of multicast data by hosts on a
   subnetwork conforms to the traditional IP multicast service model
   [<a href="#ref-2" title="PhD Thesis">2</a>].

   CBT is progressing through the IDMR working group of the IETF.  The
   CBT protocol is described in an accompanying document [<a href="#ref-1" title='"Core Based Trees (CBT version 2) Multicast Routing: Protocol Specification"'>1</a>]. For this,
   and all IDMR-related documents, see <a href="http://www.cs.ucl.ac.uk/ietf/idmr">http://www.cs.ucl.ac.uk/ietf/idmr</a>

TABLE OF CONTENTS

   <a href="#section-1">1</a>. Background...................................................  <a href="#page-2">2</a>
   <a href="#section-2">2</a>. Introduction.................................................  <a href="#page-2">2</a>
   <a href="#section-3">3</a>. Source Based Tree Algorithms.................................  <a href="#page-3">3</a>
      <a href="#section-3.1">3.1</a> Distance-Vector Multicast Algorithm......................  <a href="#page-4">4</a>
      <a href="#section-3.2">3.2</a> Link State Multicast Algorithm...........................  <a href="#page-5">5</a>
      <a href="#section-3.3">3.3</a> The Motivation for Shared Trees..........................  <a href="#page-5">5</a>
   <a href="#section-4">4</a>. CBT - The New Architecture...................................  <a href="#page-7">7</a>
      <a href="#section-4.1">4.1</a> Design Requirements......................................  <a href="#page-7">7</a>
      <a href="#section-4.2">4.2</a> Components &amp; Functions...................................  <a href="#page-8">8</a>
          <a href="#section-4.2.1">4.2.1</a> CBT Control Message Retransmission Strategy........ <a href="#page-10">10</a>
          <a href="#section-4.2.2">4.2.2</a> Non-Member Sending................................. <a href="#page-11">11</a>
   <a href="#section-5">5</a>. Interoperability with Other Multicast Routing Protocols ..... <a href="#page-11">11</a>



<span class="grey">Ballardie                     Experimental                      [Page 1]</span></pre>
<hr align="left" class="noprint" style="width: 96ex;"/><!--NewPage--><pre class="newpage"><a class="invisible" href="#page-2" id="page-2" name="page-2"> </a>
<span class="grey"><a href="rfc2201.html">RFC 2201</a>           CBT Multicast Routing Architecture     September 1997</span>


   <a href="#section-6">6</a>. Core Router Discovery........................................ <a href="#page-11">11</a>
      <a href="#section-6.1">6.1</a> Bootstrap Mechanism Overview............................. <a href="#page-12">12</a>
   <a href="#section-7">7</a>. Summary ..................................................... <a href="#page-13">13</a>
   <a href="#section-8">8</a>. Security Considerations...................................... <a href="#page-13">13</a>
   Acknowledgements ............................................... <a href="#page-14">14</a>
   References ..................................................... <a href="#page-14">14</a>
   Author Information.............................................. <a href="#page-15">15</a>

<span class="h2"><a class="dashAnchor" name="//apple_ref/Section/1.%20%20Background"></a><a class="selflink" href="#section-1" name="section-1">1</a>.  Background</span>

   Shared trees were first described by Wall in his investigation into
   low-delay approaches to broadcast and selective broadcast [<a href="#ref-3" title="Stanford University">3</a>]. Wall
   concluded that delay will not be minimal, as with shortest-path
   trees, but the delay can be kept within bounds that may be
   acceptable.  Back then, the benefits and uses of multicast were not
   fully understood, and it wasn't until much later that the IP
   multicast address space was defined (class D space [<a href="#ref-4" title='"Assigned Numbers"'>4</a>]). Deering's
   work [<a href="#ref-2" title="PhD Thesis">2</a>] in the late 1980's was pioneering in that he defined the IP
   multicast service model, and invented algorithms which allow hosts to
   arbitrarily join and leave a multicast group. All of Deering's
   multicast algorithms build source-rooted delivery trees, with one
   delivery tree per sender subnetwork. These algorithms are documented
   in [<a href="#ref-2" title="PhD Thesis">2</a>].

   After several years practical experience with multicast, we see a
   diversity of multicast applications and correspondingly, a wide
   variety of multicast application requirements.  For example,
   distributed interactive simulation (DIS) applications have strict
   requirements in terms of join latency, group membership dynamics,
   group sender populations, far exceeding the requirements of many
   other multicast applications.

   The multicast-capable part of the Internet, the MBONE, continues to
   expand rapidly.  The obvious popularity and growth of multicast means
   that the scaling aspects of wide-area multicasting cannot be
   overlooked; some predictions talk of thousands of groups being
   present at any one time in the Internet.

   We evaluate scalability in terms of network state maintenance,
   bandwidth efficiency, and protocol overhead. Other factors that can
   affect these parameters include sender set size, and wide-area
   distribution of group members.

<span class="h2"><a class="dashAnchor" name="//apple_ref/Section/2.%20%20Introduction"></a><a class="selflink" href="#section-2" name="section-2">2</a>.  Introduction</span>

   Multicasting on the local subnetwork does not require either the
   presence of a multicast router or the implementation of a multicast
   routing algorithm; on most shared media (e.g. Ethernet), a host,



<span class="grey">Ballardie                     Experimental                      [Page 2]</span></pre>
<hr align="left" class="noprint" style="width: 96ex;"/><!--NewPage--><pre class="newpage"><a class="invisible" href="#page-3" id="page-3" name="page-3"> </a>
<span class="grey"><a href="rfc2201.html">RFC 2201</a>           CBT Multicast Routing Architecture     September 1997</span>


   which need not necessarily be a group member, simply sends a
   multicast data packet, which is received by any member hosts
   connected to the same medium.

   For multicasts to extend beyond the scope of the local subnetwork,
   the subnet must have a multicast-capable router attached, which
   itself is attached (possibly "virtually") to another multicast-
   capable router, and so on. The collection of these (virtually)
   connected multicast routers forms the Internet's MBONE.

   All multicast routing protocols make use of IGMP [<a href="#ref-5">5</a>], a protocol that
   operates between hosts and multicast router(s) belonging to the same
   subnetwork. IGMP enables the subnet's multicast router(s) to monitor
   group membership presence on its directly attached links, so that if
   multicast data arrives, it knows over which of its links to send a
   copy of the packet.

   In our description of the MBONE so far, we have assumed that all
   multicast routers on the MBONE are running the same multicast routing
   protocol. In reality, this is not the case; the MBONE is a collection
   of autonomously administered multicast regions, each region defined
   by one or more multicast-capable border routers. Each region
   independently chooses to run whichever multicast routing protocol
   best suits its needs, and the regions interconnect via the "backbone
   region", which currently runs the Distance Vector Multicast Routing
   Protocol (DVMRP) [<a href="#ref-6">6</a>]. Therefore, it follows that a region's border
   router(s) must interoperate with DVMRP.

   Different algorithms use different techniques for establishing a
   distribution tree. If we classify these algorithms into source-based
   tree algorithms and shared tree algorithms, we'll see that the
   different classes have considerably different scaling
   characteristics, and the characteristics of the resulting trees
   differ too, for example, average delay. Let's look at source-based
   tree algorithms first.

<span class="h2"><a class="dashAnchor" name="//apple_ref/Section/3.%20%20Source-Based%20Tree%20Algorithms"></a><a class="selflink" href="#section-3" name="section-3">3</a>.  Source-Based Tree Algorithms</span>

   The strategy we'll use for motivating (CBT) shared tree multicast is
   based, in part, in explaining the characteristics of source-based
   tree multicast, in particular its scalability.

   Most source-based tree multicast algorithms are often referred to as
   "dense-mode" algorithms; they assume that the receiver population
   densely populates the domain of operation, and therefore the
   accompanying overhead (in terms of state, bandwidth usage, and/or
   processing costs) is justified.  Whilst this might be the case in a
   local environment, wide-area group membership tends to be sparsely



<span class="grey">Ballardie                     Experimental                      [Page 3]</span></pre>
<hr align="left" class="noprint" style="width: 96ex;"/><!--NewPage--><pre class="newpage"><a class="invisible" href="#page-4" id="page-4" name="page-4"> </a>
<span class="grey"><a href="rfc2201.html">RFC 2201</a>           CBT Multicast Routing Architecture     September 1997</span>


   distributed throughout the Internet.  There may be "pockets" of
   denseness, but if one views the global picture, wide-area groups tend
   to be sparsely distributed.

   Source-based multicast trees are either built by a distance-vector
   style algorithm, which may be implemented separately from the unicast
   routing algorithm (as is the case with DVMRP), or the multicast tree
   may be built using the information present in the underlying unicast
   routing table (as is the case with PIM-DM [<a href="#ref-7">7</a>]). The other algorithm
   used for building source-based trees is the link-state algorithm (a
   protocol instance being M-OSPF [<a href="#ref-8" title='"Multicast Extensions to OSPF"'>8</a>]).

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/3.1.%20%20Distance-Vector%20Multicast%20Algorithm"></a><a class="selflink" href="#section-3.1" name="section-3.1">3.1</a>.  Distance-Vector Multicast Algorithm</span>

   The distance-vector multicast algorithm builds a multicast delivery
   tree using a variant of the Reverse-Path Forwarding technique [<a href="#ref-9" title="21(12):1040--1048">9</a>].
   The technique basically is as follows: when a multicast router
   receives a multicast data packet, if the packet arrives on the
   interface used to reach the source of the packet, the packet is
   forwarded over all outgoing interfaces, except leaf subnets with no
   members attached.  A "leaf" subnet is one which no router would use
   to reach the souce of a multicast packet. If the data packet does not
   arrive over the link that would be used to reach the source, the
   packet is discarded.

   This constitutes a "broadcast &amp; prune" approach to multicast tree
   construction; when a data packet reaches a leaf router, if that
   router has no membership registered on any of its directly attached
   subnetworks, the router sends a prune message one hop back towards
   the source. The receiving router then checks its leaf subnets for
   group membership, and checks whether it has received a prune from all
   of its downstream routers (downstream with respect to the source).
   If so, the router itself can send a prune upstream over the interface
   leading to the source.

   The sender and receiver of a prune message must cache the &lt;source,
   group&gt; pair being reported, for a "lifetime" which is at the
   granularity of minutes. Unless a router's prune information is
   refreshed by the receipt of a new prune for &lt;source, group&gt; before
   its "lifetime" expires, that information is removed, allowing data to
   flow over the branch again. State that expires in this way is
   referred to as "soft state".

   Interestingly, routers that do not lead to group members are incurred
   the state overhead incurred by prune messages. For wide-area
   multicasting, which potentially has to support many thousands of
   active groups, each of which may be sparsely distributed, this
   technique clearly does not scale.



<span class="grey">Ballardie                     Experimental                      [Page 4]</span></pre>
<hr align="left" class="noprint" style="width: 96ex;"/><!--NewPage--><pre class="newpage"><a class="invisible" href="#page-5" id="page-5" name="page-5"> </a>
<span class="grey"><a href="rfc2201.html">RFC 2201</a>           CBT Multicast Routing Architecture     September 1997</span>


<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/3.2.%20%20Link-State%20Multicast%20Algorithm"></a><a class="selflink" href="#section-3.2" name="section-3.2">3.2</a>.  Link-State Multicast Algorithm</span>

   Routers implementing a link state algorithm periodically collect
   reachability information to their directly attached neighbours, then
   flood this throughout the routing domain in so-called link state
   update packets. Deering extended the link state algorithm for
   multicasting by having a router additionally detect group membership
   changes on its incident links before flooding this information in
   link state packets.

   Each router then, has a complete, up-to-date image of a domain's
   topology and group membership. On receiving a multicast data packet,
   each router uses its membership and topology information to calculate
   a shortest-path tree rooted at the sender subnetwork. Provided the
   calculating router falls within the computed tree, it forwards the
   data packet over the interfaces defined by its calculation. Hence,
   multicast data packets only ever traverse routers leading to members,
   either directly attached, or further downstream. That is, the
   delivery tree is a true multicast tree right from the start.

   However, the flooding (reliable broadcasting) of group membership
   information is the predominant factor preventing the link state
   multicast algorithm being applicable over the wide-area.  The other
   limiting factor is the processing cost of the Dijkstra calculation to
   compute the shortest-path tree for each active source.

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/3.3.%20%20The%20Motivation%20for%20Shared%20Trees"></a><a class="selflink" href="#section-3.3" name="section-3.3">3.3</a>.  The Motivation for Shared Trees</span>

   The algorithms described in the previous sections clearly motivate
   the need for a multicast algorithm(s) that is more scalable. CBT was
   designed primarily to address the topic of scalability; a shared tree
   architecture offers an improvement in scalability over source tree
   architectures by a factor of the number of active sources (where
   source is usually a subnetwork aggregate).  Source trees scale O(S *
   G), since a distinct delivery tree is built per active source. Shared
   trees eliminate the source (S) scaling factor; all sources use the
   same shared tree, and hence a shared tree scales O(G).  The
   implication of this is that applications with many active senders,
   such as distributed interactive simulation applications, and
   distributed video-gaming (where most receivers are also senders),
   have a significantly lesser impact on underlying multicast routing if
   shared trees are used.









<span class="grey">Ballardie                     Experimental                      [Page 5]</span></pre>
<hr align="left" class="noprint" style="width: 96ex;"/><!--NewPage--><pre class="newpage"><a class="invisible" href="#page-6" id="page-6" name="page-6"> </a>
<span class="grey"><a href="rfc2201.html">RFC 2201</a>           CBT Multicast Routing Architecture     September 1997</span>


   In the "back of the envelope" table below we compare the amount of
   state required by CBT and DVMRP for different group sizes with
   different numbers of active sources:

  |--------------|---------------------------------------------------|
  |  Number of   |                |                |                 |
  |    groups    |        10      |       100      |        1000     |
  ====================================================================
  |  Group size  |                |                |                 |
  | (# members)  |        20      |       40       |         60      |
  -------------------------------------------------------------------|
  | No. of srcs  |    |     |     |    |     |     |    |     |      |
  |  per group   |10% | 50% |100% |10% | 50% |100% |10% | 50% | 100% |
  --------------------------------------------------------------------
  | No. of DVMRP |    |     |     |    |     |     |    |     |      |
  |    router    |    |     |     |    |     |     |    |     |      |
  |   entries    | 20 | 100 | 200 |400 | 2K  | 4K  | 6K | 30K | 60K  |
  --------------------------------------------------------------------
  | No. of CBT   |                |                |                 |
  |  router      |                |                |                 |
  |  entries     |       10       |       100      |       1000      |
  |------------------------------------------------------------------|

           Figure 1: Comparison of DVMRP and CBT Router State

   Shared trees also incur significant bandwidth and state savings
   compared with source trees; firstly, the tree only spans a group's
   receivers (including links/routers leading to receivers) -- there is
   no cost to routers/links in other parts of the network. Secondly,
   routers between a non-member sender and the delivery tree are not
   incurred any cost pertaining to multicast, and indeed, these routers
   need not even be multicast-capable -- packets from non-member senders
   are encapsulated and unicast to a core on the tree.


















<span class="grey">Ballardie                     Experimental                      [Page 6]</span></pre>
<hr align="left" class="noprint" style="width: 96ex;"/><!--NewPage--><pre class="newpage"><a class="invisible" href="#page-7" id="page-7" name="page-7"> </a>
<span class="grey"><a href="rfc2201.html">RFC 2201</a>           CBT Multicast Routing Architecture     September 1997</span>


   The figure below illustrates a core based tree.

           b      b     b-----b
            \     |     |
             \    |     |
              b---b     b------b
             /     \  /                   KEY....
            /       \/
           b         X---b-----b          X = Core
                    / \                   b = on-tree router
                   /   \
                  /     \
                  b      b------b
                 / \     |
                /   \    |
               b     b   b

                           Figure 2: CBT Tree

<span class="h2"><a class="dashAnchor" name="//apple_ref/Section/4.%20%20CBT%20-%20The%20New%20Architecture"></a><a class="selflink" href="#section-4" name="section-4">4</a>.  CBT - The New Architecture</span>

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/4.1.%20%20Design%20Requirements"></a><a class="selflink" href="#section-4.1" name="section-4.1">4.1</a>.  Design Requirements</span>

   The CBT shared tree design was geared towards several design
   objectives:

   o    scalability - the CBT designers decided not to sacrifice CBT's
        O(G) scaling characteric to optimize delay using SPTs, as does
        PIM.  This was an important design decision, and one, we think,
        was taken with foresight; once multicasting becomes ubiquitous,
        router state maintenance will be a predominant scaling factor.
        It is possible in some circumstances to improve/optimize the
        delay of shared trees by other means. For example, a broadcast-
        type lecture with a single sender (or limited set of
        infrequently changing senders) could have its core placed in the
        locality of the sender, allowing the CBT to emulate a shortest-
        path tree (SPT) whilst still maintaining its O(G) scaling
        characteristic. More generally, because CBT does not incur
        source-specific state, it is particularly suited to many sender
        applications.

   o    robustness - source-based tree algorithms are clearly robust; a
        sender simply sends its data, and intervening routers "conspire"
        to get the data where it needs to, creating state along the way.
        This is the so-called "data driven" approach -- there is no
        set-up protocol involved.





<span class="grey">Ballardie                     Experimental                      [Page 7]</span></pre>
<hr align="left" class="noprint" style="width: 96ex;"/><!--NewPage--><pre class="newpage"><a class="invisible" href="#page-8" id="page-8" name="page-8"> </a>
<span class="grey"><a href="rfc2201.html">RFC 2201</a>           CBT Multicast Routing Architecture     September 1997</span>


        It is not as easy to achieve the same degree of robustness in
        shared tree algorithms; a shared tree's core router maintains
        connectivity between all group members, and is thus a single
        point of failure.  Protocol mechanisms must be present that
        ensure a core failure is detected quickly, and the tree
        reconnected quickly using a replacement core router.

   o    simplicity - the CBT protocol is relatively simple compared to
        most other multicast routing protocols. This simplicity can lead
        to enhanced performance compared to other protocols.

   o    interoperability - from a multicast perspective, the Internet is
        a collection of heterogeneous multicast regions. The protocol
        interconnecting these multicast regions is currently DVMRP [<a href="#ref-6">6</a>];
        any regions not running DVMRP connect to the DVMRP "backbone" as
        stub regions.  CBT has well-defined interoperability mechanisms
        with DVMRP [<a href="#ref-15">15</a>].

<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/4.2.%20%20CBT%20Components%20%26%20Functions"></a><a class="selflink" href="#section-4.2" name="section-4.2">4.2</a>.  CBT Components &amp; Functions</span>

   The CBT protocol is designed to build and maintain a shared multicast
   distribution tree that spans only those networks and links leading to
   interested receivers.

   To achieve this, a host first expresses its interest in joining a
   group by multicasting an IGMP host membership report [<a href="#ref-5">5</a>] across its
   attached link. On receiving this report, a local CBT aware router
   invokes the tree joining process (unless it has already) by
   generating a JOIN_REQUEST message, which is sent to the next hop on
   the path towards the group's core router (how the local router
   discovers which core to join is discussed in <a href="#section-6">section 6</a>). This join
   message must be explicitly acknowledged (JOIN_ACK) either by the core
   router itself, or by another router that is on the unicast path
   between the sending router and the core, which itself has already
   successfully joined the tree.

   The join message sets up transient join state in the routers it
   traverses, and this state consists of &lt;group, incoming interface,
   outgoing interface&gt;. "Incoming interface" and "outgoing interface"
   may be "previous hop" and "next hop", respectively, if the
   corresponding links do not support multicast transmission. "Previous
   hop" is taken from the incoming control packet's IP source address,
   and "next hop" is gleaned from the routing table - the next hop to
   the specified core address. This transient state eventually times out
   unless it is "confirmed" with a join acknowledgement (JOIN_ACK) from
   upstream. The JOIN_ACK traverses the reverse path of the
   corresponding join message, which is possible due to the presence of
   the transient join state.  Once the acknowledgement reaches the



<span class="grey">Ballardie                     Experimental                      [Page 8]</span></pre>
<hr align="left" class="noprint" style="width: 96ex;"/><!--NewPage--><pre class="newpage"><a class="invisible" href="#page-9" id="page-9" name="page-9"> </a>
<span class="grey"><a href="rfc2201.html">RFC 2201</a>           CBT Multicast Routing Architecture     September 1997</span>


   router that originated the join message, the new receiver can receive
   traffic sent to the group.

   Loops cannot be created in a CBT tree because a) there is only one
   active core per group, and b) tree building/maintenance scenarios
   which may lead to the creation of tree loops are avoided.  For
   example, if a router's upstream neighbour becomes unreachable, the
   router immediately "flushes" all of its downstream branches, allowing
   them to individually rejoin if necessary.  Transient unicast loops do
   not pose a threat because a new join message that loops back on
   itself will never get acknowledged, and thus eventually times out.

   The state created in routers by the sending or receiving of a
   JOIN_ACK is bi-directional - data can flow either way along a tree
   "branch", and the state is group specific - it consists of the group
   address and a list of local interfaces over which join messages for
   the group have previously been acknowledged. There is no concept of
   "incoming" or "outgoing" interfaces, though it is necessary to be
   able to distinguish the upstream interface from any downstream
   interfaces. In CBT, these interfaces are known as the "parent" and
   "child" interfaces, respectively.

   With regards to the information contained in the multicast forwarding
   cache, on link types not supporting native multicast transmission an
   on-tree router must store the address of a parent and any children.
   On links supporting multicast however, parent and any child
   information is represented with local interface addresses (or similar
   identifying information, such as an interface "index") over which the
   parent or child is reachable.

   When a multicast data packet arrives at a router, the router uses the
   group address as an index into the multicast forwarding cache. A copy
   of the incoming multicast data packet is forwarded over each
   interface (or to each address) listed in the entry except the
   incoming interface.

   Each router that comprises a CBT multicast tree, except the core
   router, is responsible for maintaining its upstream link, provided it
   has interested downstream receivers, i.e. the child interface list is
   not NULL. A child interface is one over which a member host is
   directly attached, or one over which a downstream on-tree router is
   attached.  This "tree maintenance" is achieved by each downstream
   router periodically sending a "keepalive" message (ECHO_REQUEST) to
   its upstream neighbour, i.e. its parent router on the tree. One
   keepalive message is sent to represent entries with the same parent,
   thereby improving scalability on links which are shared by many
   groups.  On multicast capable links, a keepalive is multicast to the
   "all-cbt-routers" group (IANA assigned as 224.0.0.15); this has a



<span class="grey">Ballardie                     Experimental                      [Page 9]</span></pre>
<hr align="left" class="noprint" style="width: 96ex;"/><!--NewPage--><pre class="newpage"><a class="invisible" href="#page-10" id="page-10" name="page-10"> </a>
<span class="grey"><a href="rfc2201.html">RFC 2201</a>           CBT Multicast Routing Architecture     September 1997</span>


   suppressing effect on any other router for which the link is its
   parent link.  If a parent link does not support multicast
   transmission, keepalives are unicast.

   The receipt of a keepalive message over a valid child interface
   immediately prompts a response (ECHO_REPLY), which is either unicast
   or multicast, as appropriate.

   The ECHO_REQUEST does not contain any group information; the
   ECHO_REPLY does, but only periodically. To maintain consistent
   information between parent and child, the parent periodically
   reports, in a ECHO_REPLY, all groups for which it has state, over
   each of its child interfaces for those groups. This group-carrying
   echo reply is not prompted explicitly by the receipt of an echo
   request message.  A child is notified of the time to expect the next
   echo reply message containing group information in an echo reply
   prompted by a child's echo request. The frequency of parent group
   reporting is at the granularity of minutes.

   It cannot be assumed all of the routers on a multi-access link have a
   uniform view of unicast routing; this is particularly the case when a
   multi-access link spans two or more unicast routing domains. This
   could lead to multiple upstream tree branches being formed (an error
   condition) unless steps are taken to ensure all routers on the link
   agree which is the upstream router for a particular group. CBT
   routers attached to a multi-access link participate in an explicit
   election mechanism that elects a single router, the designated router
   (DR), as the link's upstream router for all groups. Since the DR
   might not be the link's best next-hop for a particular core router,
   this may result in join messages being re-directed back across a
   multi-access link. If this happens, the re-directed join message is
   unicast across the link by the DR to the best next-hop, thereby
   preventing a looping scenario.  This re-direction only ever applies
   to join messages.  Whilst this is suboptimal for join messages, which
   are generated infrequently, multicast data never traverses a link
   more than once (either natively, or encapsulated).

   In all but the exception case described above, all CBT control
   messages are multicast over multicast supporting links to the "all-
   cbt-routers" group, with IP TTL 1. When a CBT control message is sent
   over a non-multicast supporting link, it is explicitly addressed to
   the appropriate next hop.

<span class="h4"><a class="dashAnchor" name="//apple_ref/Section/4.2.1.%20%20CBT%20Control%20Message%20Retransmission%20Strategy"></a><a class="selflink" href="#section-4.2.1" name="section-4.2.1">4.2.1</a>.  CBT Control Message Retransmission Strategy</span>

   Certain CBT control messages illicit a response of some sort. Lack of
   response may be due to an upstream router crashing, or the loss of
   the original message, or its response. To detect these events, CBT



<span class="grey">Ballardie                     Experimental                     [Page 10]</span></pre>
<hr align="left" class="noprint" style="width: 96ex;"/><!--NewPage--><pre class="newpage"><a class="invisible" href="#page-11" id="page-11" name="page-11"> </a>
<span class="grey"><a href="rfc2201.html">RFC 2201</a>           CBT Multicast Routing Architecture     September 1997</span>


   retransmits those control messages for which it expects a response,
   if that response is not forthcoming within the retransmission-
   interval, which varies depending on the type of message involved.
   There is an upper bound (typically 3) on the number of
   retransmissions of the original message before an exception condition
   is raised.

   For example, the exception procedure for lack of response to an
   ECHO_REQUEST is to send a QUIT_NOTIFICATION upstream and a FLUSH_TREE
   message downstream for the group. If this is router has group members
   attached, it restarts the joining process to the group's core.

<span class="h4"><a class="dashAnchor" name="//apple_ref/Section/4.2.2.%20%20Non-Member%20Sending"></a><a class="selflink" href="#section-4.2.2" name="section-4.2.2">4.2.2</a>.  Non-Member Sending</span>

   If a non-member sender's local router is already on-tree for the
   group being sent to, the subnet's upstream router simply forwards the
   data packet over all outgoing interfaces corresponding to that
   group's forwarding cache entry. This is in contrast to PIM-SM [<a href="#ref-18" title='"Protocol Independent Multicast-Sparse Mode (PIM-SM): Protocol Specification"'>18</a>]
   which must encapsulate data from a non-member sender, irrespective of
   whether the local router has joined the tree. This is due to PIM's
   uni-directional state.

   If the sender's subnet is not attached to the group tree, the local
   DR must encapsulate the data packet and unicast it to the group's
   core router, where it is decapsulated and disseminated over all tree
   interfaces, as specified by the core's forwarding cache entry for the
   group. The data packet encapsulation method is IP-in-IP [<a href="#ref-14" title='"IP Encapsulation within IP"'>14</a>].

   Routers in between a non-member sender and the group's core need not
   know anything about the multicast group, and indeed may even be
   multicast-unaware. This makes CBT particulary attractive for
   applications with non-member senders.

<span class="h2"><a class="dashAnchor" name="//apple_ref/Section/5.%20%20Interoperability%20with%20Other%20Multicast%20Routing%20Protocols"></a><a class="selflink" href="#section-5" name="section-5">5</a>.  Interoperability with Other Multicast Routing Protocols</span>

   See "interoperability" in <a href="#section-4.1">section 4.1</a>.

   The interoperability mechanisms for interfacing CBT with DVMRP are
   defined in [<a href="#ref-15">15</a>].

<span class="h2"><a class="dashAnchor" name="//apple_ref/Section/6.%20%20Core%20Router%20Discovery"></a><a class="selflink" href="#section-6" name="section-6">6</a>.  Core Router Discovery</span>

   Core router discovery is by far the most controversial and difficult
   aspect of shared tree multicast architectures, particularly in the
   context of inter-domain multicast routing (IDMR).  There have been
   many proposals over the past three years or so, including advertising
   core addresses in a multicast session directory like "sdr" [<a href="#ref-11">11</a>],
   manual placement, and the HPIM [<a href="#ref-12" title="J. Crowcroft">12</a>] approach of strictly dividing up



<span class="grey">Ballardie                     Experimental                     [Page 11]</span></pre>
<hr align="left" class="noprint" style="width: 96ex;"/><!--NewPage--><pre class="newpage"><a class="invisible" href="#page-12" id="page-12" name="page-12"> </a>
<span class="grey"><a href="rfc2201.html">RFC 2201</a>           CBT Multicast Routing Architecture     September 1997</span>


   the multicast address space into many "hierarchical scopes" and using
   explicit advertising of core routers between scope levels.

   There are currently two options for CBTv2 [<a href="#ref-1" title='"Core Based Trees (CBT version 2) Multicast Routing: Protocol Specification"'>1</a>] core discovery; the
   "bootstrap" mechamism, and manual placement. The bootstrap mechanisms
   (as currently specified with the PIM sparse mode protocol [<a href="#ref-18" title='"Protocol Independent Multicast-Sparse Mode (PIM-SM): Protocol Specification"'>18</a>]) is
   applicable only to intra-domain core discovery, and allows for a
   "plug &amp; play" type operation with minimal configuration. The
   disadvantage of the bootstrap mechanism is that it is much more
   difficult to affect the shape, and thus optimality, of the resulting
   distribution tree. Also, it must be implemented by all CBT routers
   within a domain.

   Manual configuration of leaf routers with &lt;core, group&gt; mappings is
   the other option (note: leaf routers only); this imposes a degree of
   administrative burden - the mapping for a particular group must be
   coordinated across all leaf routers to ensure consistency. Hence,
   this method does not scale particularly well. However, it is likely
   that "better" trees will result from this method, and it is also the
   only available option for inter-domain core discovery currently
   available.


<span class="h3"><a class="dashAnchor" name="//apple_ref/Section/6.1.%20%20Bootstrap%20Mechanism%20Overview"></a><a class="selflink" href="#section-6.1" name="section-6.1">6.1</a>.  Bootstrap Mechanism Overview</span>

   It is unlikely at this stage that the bootstrap mechanism will be
   appended to a well-known network layer protocol, such as IGMP [<a href="#ref-5">5</a>] or
   ICMP [<a href="#ref-13" title='"Internet Control Message Protocol (ICMP)"'>13</a>], though this would facilitate its ubiquitous (intra-domain)
   deployment.  Therefore, each multicast routing protocol requiring the
   bootstrap mechanism must implement it as part of the multicast
   routing protocol itself.

   A summary of the operation of the bootstrap mechanism follows. It is
   assumed that all routers within the domain implement the "bootstrap"
   protocol, or at least forward bootstrap protocol messages.

   A subset of the domain's routers are configured to be CBT candidate
   core routers. Each candidate core router periodically (default every
   60 secs) advertises itself to the domain's Bootstrap Router (BSR),
   using  "Core Advertisement" messages.  The BSR is itself elected
   dynamically from all (or participating) routers in the domain.  The
   domain's elected BSR collects "Core Advertisement" messages from
   candidate core routers and periodically advertises a candidate core
   set (CC-set) to each other router in the domain, using traditional
   hopby-hop unicast forwarding. The BSR uses "Bootstrap Messages" to
   advertise the CC-set. Together, "Core Advertisements" and "Bootstrap
   Messages" comprise the "bootstrap" protocol.




<span class="grey">Ballardie                     Experimental                     [Page 12]</span></pre>
<hr align="left" class="noprint" style="width: 96ex;"/><!--NewPage--><pre class="newpage"><a class="invisible" href="#page-13" id="page-13" name="page-13"> </a>
<span class="grey"><a href="rfc2201.html">RFC 2201</a>           CBT Multicast Routing Architecture     September 1997</span>


   When a router receives an IGMP host membership report from one of its
   directly attached hosts, the local router uses a hash function on the
   reported group address, the result of which is used as an index into
   the CC-set. This is how local routers discover which core to use for
   a particular group.

   Note the hash function is specifically tailored such that a small
   number of consecutive groups always hash to the same core.
   Furthermore, bootstrap messages can carry a "group mask", potentially
   limiting a CC-set to a particular range of groups. This can help
   reduce traffic concentration at the core.

   If a BSR detects a particular core as being unreachable (it has not
   announced its availability within some period), it deletes the
   relevant core from the CC-set sent in its next bootstrap message.
   This is how a local router discovers a group's core is unreachable;
   the router must re-hash for each affected group and join the new core
   after removing the old state. The removal of the "old" state follows
   the sending of a QUIT_NOTIFICATION upstream, and a FLUSH_TREE message
   downstream.

<span class="h2"><a class="dashAnchor" name="//apple_ref/Section/7.%20%20Summary"></a><a class="selflink" href="#section-7" name="section-7">7</a>.  Summary</span>

   This document presents an architecture for intra- and inter-domain
   multicast routing.  We motivated this architecture by describing how
   an inter-domain multicast routing algorithm must scale to large
   numbers of groups present in the internetwork, and discussed why most
   other existing algorithms are less suited to inter-domain multicast
   routing.  We followed by describing the features and components of
   the architecture, illustrating its simplicity and scalability.

<span class="h2"><a class="dashAnchor" name="//apple_ref/Section/8.%20%20Security%20Considerations"></a><a class="selflink" href="#section-8" name="section-8">8</a>.  Security Considerations</span>

   Security considerations are not addressed in this memo.

   Whilst multicast security is a topic of ongoing research, multicast
   applications (users) nevertheless have the ability to take advantage
   of security services such as encryption or/and authentication
   provided such services are supported by the applications.

   RFCs 1949 and 2093/2094 discuss different ways of distributing
   multicast key material, which can result in the provision of network
   layer access control to a multicast distribution tree.

   [<a id="ref-19" name="ref-19">19</a>] offers a synopsis of multicast security threats and proposes
   some possible counter measures.





<span class="grey">Ballardie                     Experimental                     [Page 13]</span></pre>
<hr align="left" class="noprint" style="width: 96ex;"/><!--NewPage--><pre class="newpage"><a class="invisible" href="#page-14" id="page-14" name="page-14"> </a>
<span class="grey"><a href="rfc2201.html">RFC 2201</a>           CBT Multicast Routing Architecture     September 1997</span>


   Beyond these, little published work exists on the topic of multicast
   security.

Acknowledgements

   Special thanks goes to Paul Francis, NTT Japan, for the original
   brainstorming sessions that brought about this work.

   Clay Shields' work on OCBT [<a href="#ref-17" title="Japan">17</a>] identified various failure scenarios
   with a multi-core architecture, resulting in the specification of a
   single core architecture.

   Others that have contributed to the progress of CBT include Ken
   Carlberg, Eric Crawley, Jon Crowcroft, Mark Handley, Ahmed Helmy,
   Nitin Jain, Alan O'Neill, Steven Ostrowsksi, Radia Perlman, Scott
   Reeve, Benny Rodrig, Martin Tatham, Dave Thaler, Sue Thompson, Paul
   White, and other participants of the IETF IDMR working group.

   Thanks also to 3Com Corporation and British Telecom Plc for funding
   this work.

References

   [<a id="ref-1" name="ref-1">1</a>] Ballardie, A., "Core Based Trees (CBT version 2) Multicast
   Routing: Protocol Specification", <a href="rfc2189.html">RFC 2189</a>, September 1997.

   [<a id="ref-2" name="ref-2">2</a>] Multicast Routing in a Datagram Internetwork; S. Deering, PhD
   Thesis, 1991; <a href="ftp://gregorio.stanford.edu/vmtp/sd-thesis.ps">ftp://gregorio.stanford.edu/vmtp/sd-thesis.ps</a>.

   [<a id="ref-3" name="ref-3">3</a>] Mechanisms for Broadcast and Selective Broadcast; D. Wall; PhD
   thesis, Stanford University, June 1980. Technical Report #90.

   [<a id="ref-4" name="ref-4">4</a>] Reynolds, J., and J. Postel, "Assigned Numbers", STD 2, <a href="rfc1700.html">RFC 1700</a>,
   October 1994.

   [<a id="ref-5" name="ref-5">5</a>] Internet Group Management Protocol, version 2 (IGMPv2); W.
   Fenner; Work In Progress.

   [<a id="ref-6" name="ref-6">6</a>] Distance Vector Multicast Routing Protocol (DVMRP); T. Pusateri;
   Work In Progress.

   [<a id="ref-7" name="ref-7">7</a>] Protocol Independent Multicast (PIM) Dense Mode Specification; D.
   Estrin et al; <a href="ftp://netweb.usc.edu/pim">ftp://netweb.usc.edu/pim</a>, Work In Progress.

   [<a id="ref-8" name="ref-8">8</a>] Moy, J., "Multicast Extensions to OSPF", <a href="rfc1584.html">RFC 1584</a>, March 1994.

   [<a id="ref-9" name="ref-9">9</a>] Reverse path forwarding of  broadcast packets; Y.K. Dalal and
   R.M.  Metcalfe; Communications of the ACM, 21(12):1040--1048, 1978.



<span class="grey">Ballardie                     Experimental                     [Page 14]</span></pre>
<hr align="left" class="noprint" style="width: 96ex;"/><!--NewPage--><pre class="newpage"><a class="invisible" href="#page-15" id="page-15" name="page-15"> </a>
<span class="grey"><a href="rfc2201.html">RFC 2201</a>           CBT Multicast Routing Architecture     September 1997</span>


   [<a id="ref-10" name="ref-10">10</a>] Some Issues for an Inter-Domain Multicast Routing Protocol; D.
   Meyer;  Work In Progress.

   [<a id="ref-11" name="ref-11">11</a>] SDP: Session Description Protocol; M. Handley and V. Jacobson;
   Work In Progress.

   [<a id="ref-12" name="ref-12">12</a>] Hierarchical Protocol Independent Multicast; M. Handley, J.
   Crowcroft, I. Wakeman.  Available from:
   <a href="http://www.cs.ucl.ac.uk/staff/M.Handley/hpim.ps">http://www.cs.ucl.ac.uk/staff/M.Handley/hpim.ps</a>  and
   <a href="ftp://cs.ucl.ac.uk/darpa/IDMR/hpim.ps">ftp://cs.ucl.ac.uk/darpa/IDMR/hpim.ps</a>   Work done 1995.

   [<a id="ref-13" name="ref-13">13</a>] Postel, J., "Internet Control Message Protocol (ICMP)", STD 5,
   <a href="rfc792.html">RFC 792</a>, September 1981.

   [<a id="ref-14" name="ref-14">14</a>] Perkins, C., "IP Encapsulation within IP", <a href="rfc2003.html">RFC 2003</a>, October
   1996.

   [<a id="ref-15" name="ref-15">15</a>] CBT - Dense Mode Multicast Interoperability; A. Ballardie; Work
   In Progress.

   [<a id="ref-16" name="ref-16">16</a>] Performance and Resource Cost Comparisons of Multicast Routing
   Algorithms for Distributed Interactive Simulation Applications; T.
   Billhartz, J. Bibb Cain, E.  Farrey-Goudreau, and D. Feig. Available
   from: <a href="http://www.epm.ornl.gov/~sgb/pubs">http://www.epm.ornl.gov/~sgb/pubs</a>.html; July 1995.

   [<a id="ref-17" name="ref-17">17</a>] The Ordered Core Based Tree Protocol; C. Shields and J.J.
   Garcia- Luna-Aceves; In Proceedings of IEEE Infocom'97, Kobe, Japan,
   April 1997; <a href="http://www.cse.ucsc.edu/research/ccrg/publications/info-comm97ocbt.ps.gz">http://www.cse.ucsc.edu/research/ccrg/publications/info-</a>
   <a href="http://www.cse.ucsc.edu/research/ccrg/publications/info-comm97ocbt.ps.gz">comm97ocbt.ps.gz</a>

   [<a id="ref-18" name="ref-18">18</a>] Estrin, D., et. al., "Protocol Independent Multicast-Sparse Mode
   (PIM-SM): Protocol Specification", <a href="rfc2117.html">RFC 2117</a>, June 1997.

   [<a id="ref-19" name="ref-19">19</a>] Multicast-Specific Security Threats and Counter-Measures; A.
   Ballardie and J. Crowcroft; In Proceedings "Symposium on Network and
   Distributed System Security", February 1995, pp.2-16.

Author Information

   Tony Ballardie,
   Research Consultant

   EMail: ABallardie@acm.org








Ballardie                     Experimental                     [Page 15]

</pre><br/>
    <span class="noprint"><small><small>Html markup produced by rfcmarkup 1.126, available from
      <a href="https://tools.ietf.org/tools/rfcmarkup/">https://tools.ietf.org/tools/rfcmarkup/</a>
    </small></small></span>
  </div>




</body><!-- Mirrored from tools.ietf.org/html/rfc2201 by HTTrack Website Copier/3.x [XR&CO'2014], Tue, 06 Mar 2018 23:18:54 GMT --></html>